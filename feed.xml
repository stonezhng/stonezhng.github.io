<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://stonezhng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://stonezhng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-12T15:37:55+00:00</updated><id>https://stonezhng.github.io/feed.xml</id><title type="html">blank</title><subtitle>Hi this is Sidong (or you can just call me Stone). I am recording my research projects, ideas and learning notes in this website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Uploading Custom Dataset to HuggingFace with Croissant Metafile</title><link href="https://stonezhng.github.io/blog/2025/Uploading-Custom-Dataset-to-HuggingFace-with-Croissant-Metafile/" rel="alternate" type="text/html" title="Uploading Custom Dataset to HuggingFace with Croissant Metafile"/><published>2025-05-12T07:00:00+00:00</published><updated>2025-05-12T07:00:00+00:00</updated><id>https://stonezhng.github.io/blog/2025/Uploading-Custom-Dataset-to-HuggingFace-with-Croissant-Metafile</id><content type="html" xml:base="https://stonezhng.github.io/blog/2025/Uploading-Custom-Dataset-to-HuggingFace-with-Croissant-Metafile/"><![CDATA[<h1 id="why-this-matters">Why this Matters</h1> <p>HuggingFace has a whole system to reformat the uploaded dataset. If you do not follow it you bascially can just use it like an online drive, but if you follow all the requirements to upload your dataset formally it can basically automatically convert your data to parquet format, and generate a fancy dataset card, as well as the croissant metafile, as the meta information of the dataset.</p> <p>I want to use the ImageFolder format from the two formats supported by HuggingFace. However, when searching on how to do this from scratch I do feel the offical tutorials are quite limited, like the examples are mostly about data of strings or images. It is 2025 and many are using preprocessed features from the raw data, so the inputs may be just a 512 or 768 dim vector. Instructions on uploading these types of data are not that clear so I write to record my own experience. My final uploaded dataset can be found here: https://huggingface.co/datasets/stonezh/PairedMNIST</p> <h1 id="local-file-structure">Local File Structure</h1> <p>Let us assume the local file is organized in this way:</p> <p>📁 test<br/> ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label1.npy<br/> │ └── …<br/> ├── 📄 metadata.jsonl<br/> ├── 📁 second_img</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── img0.npy  
├── img1.npy  
└── ...    📁 train   ├── 📁 first_img   │   ├── img0.npy   │   ├── img1.npy   │   └── ...   ├── 📁 label   │   ├── label0.npy   │   ├── label1.npy   │   └── ...   ├── 📄 metadata.jsonl   ├── 📁 second_img 

├── img0.npy  
├── img1.npy  
└── ...   📁 val  ├── 📁 first_img   │   ├── img0.npy   │   ├── img1.npy   │   └── ...   ├── 📁 label   │   ├── label0.npy   │   ├── label1.npy   │   └── ...   ├── 📄 metadata.jsonl   ├── 📁 second_img 

├── img0.npy  
├── img1.npy  
└── ...  
</code></pre></div></div> <p>This is the backbone structure of ImageFolder. The data itself is basically just MNIST with two images, and label being the multiplication of the two figures in the two images, but instead of assuming the image is an image and the label is a label, I d rather just treat them as an 1D vector, as if they are preprocessed latent features. So each imgX.npy is just a 784 dim vector, and each label is just a 1 dim vector.</p> <h1 id="step-1-create-local-metafile">Step 1: Create Local Metafile</h1> <p>Just write a local python script to scan the files and get a meta file. I recommend using <code class="language-plaintext highlighter-rouge">jsonlines</code> package to save the metafile as <code class="language-plaintext highlighter-rouge">.jsonl</code> file. Suppose we call the metafile as <code class="language-plaintext highlighter-rouge">metafile.jsonl</code> (name does not matter you can name it anything you like), it should look like this:</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"first_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"first_img/img0.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"second_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"second_img/img0.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"label/label0.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"this is the 0th sample"</span><span class="p">}</span><span class="w">
</span><span class="p">{</span><span class="nl">"first_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"first_img/img1.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"second_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"second_img/img1.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"label/label1.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"this is the 1th sample"</span><span class="p">}</span><span class="w">
</span><span class="p">{</span><span class="nl">"first_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"first_img/img2.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"second_img"</span><span class="p">:</span><span class="w"> </span><span class="s2">"second_img/img2.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"label/label2.npy"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"this is the 2th sample"</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>The folder now should look like this</p> <p>📁 test<br/> ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy <br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label0.npy <br/> │ └── …<br/> ├── 📄 metadata.jsonl<br/> ├── 📁 second_img<br/> ├── img0.npy<br/> ├── img1.npy <br/> └── …<br/> 📁 train<br/> ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy <br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label0.npy <br/> │ └── …<br/> ├── 📄 metadata.jsonl<br/> ├── 📁 second_img<br/> ├── img0.npy<br/> ├── img1.npy <br/> └── …<br/> 📁 val ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy <br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label0.npy <br/> │ └── …<br/> ├── 📄 metadata.jsonl<br/> ├── 📁 second_img<br/> ├── img0.npy<br/> ├── img1.npy <br/> └── …</p> <h1 id="write-custom-datasetgeneratorbasedbuilder">Write Custom dataset.GeneratorBasedBuilder</h1> <p>Under each folder (train, val, test), put a corresponding python file named train.py, val.py and test.py respectively. For test.py it looks like</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_url</span>
<span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="n">METADATA_URL</span> <span class="o">=</span> <span class="nf">hf_hub_url</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">stonezh/PairedMNIST</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">metadata.jsonl</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">repo_type</span><span class="o">=</span><span class="sh">"</span><span class="s">dataset</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">_DESCRIPTION</span> <span class="o">=</span> <span class="sh">"</span><span class="s">TODO</span><span class="sh">"</span>
<span class="n">_HOMEPAGE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">NONE</span><span class="sh">"</span>
<span class="n">_LICENSE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">NONE</span><span class="sh">"</span>
<span class="n">_CITATION</span> <span class="o">=</span> <span class="sh">"</span><span class="s">NONE</span><span class="sh">"</span>

<span class="c1"># _FEATURES = datasets.Features(
#     {
#         "first_image": datasets.Sequence(datasets.Value("float64"), length=784),  # https://discuss.huggingface.co/t/setting-dataset-feature-value-as-numpy-array/20940/2
#         "second_image": datasets.Sequence(datasets.Value("float64"), length=784),
#         "label": datasets.Sequence(datasets.Value("float64"), length=1)
#     },
# )
</span>
<span class="n">_FEATURES</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">Features</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">first_image</span><span class="sh">"</span><span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">Array2D</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">float64</span><span class="sh">'</span><span class="p">),</span>  <span class="c1"># https://discuss.huggingface.co/t/setting-dataset-feature-value-as-numpy-array/20940/2
</span>        <span class="sh">"</span><span class="s">second_image</span><span class="sh">"</span><span class="p">:</span>  <span class="n">datasets</span><span class="p">.</span><span class="nc">Array2D</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">float64</span><span class="sh">'</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">:</span>  <span class="n">datasets</span><span class="p">.</span><span class="nc">Array2D</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="sh">'</span><span class="s">float64</span><span class="sh">'</span><span class="p">)</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="c1"># _FEATURES = datasets.Features(
#     {
#         "first_image": datasets.Image,  # https://discuss.huggingface.co/t/setting-dataset-feature-value-as-numpy-array/20940/2
#         "second_image": datasets.Image,
#         "label": datasets.Image
#     },
# )
</span>
<span class="n">_DEFAULT_CONFIG</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">BuilderConfig</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">default</span><span class="sh">"</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">datasets</span><span class="p">.</span><span class="nc">Version</span><span class="p">(</span><span class="sh">"</span><span class="s">0.0.1</span><span class="sh">"</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">PairedMNIST</span><span class="p">(</span><span class="n">datasets</span><span class="p">.</span><span class="n">GeneratorBasedBuilder</span><span class="p">):</span>
    <span class="n">BUILDER_CONFIGS</span> <span class="o">=</span> <span class="p">[</span><span class="n">_DEFAULT_CONFIG</span><span class="p">]</span>
    <span class="n">DEFAULT_CONFIG_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">default</span><span class="sh">"</span>

    <span class="k">def</span> <span class="nf">_info</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">DatasetInfo</span><span class="p">(</span>
            <span class="n">description</span><span class="o">=</span><span class="n">_DESCRIPTION</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">_FEATURES</span><span class="p">,</span>
            <span class="n">supervised_keys</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">homepage</span><span class="o">=</span><span class="n">_HOMEPAGE</span><span class="p">,</span>
            <span class="n">license</span><span class="o">=</span><span class="n">_LICENSE</span><span class="p">,</span>
            <span class="n">citation</span><span class="o">=</span><span class="n">_CITATION</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_split_generators</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dl_manager</span><span class="p">):</span>
        <span class="c1"># print(dl_manager._data_dir)
</span>        <span class="c1"># metadata_path = os.path.join(dl_manager._data_dir, "metadata.jsonl")
</span>        <span class="c1"># first_img_path = os.path.join(dl_manager._data_dir, "first_img")
</span>        <span class="c1"># second_img_path = os.path.join(dl_manager._data_dir, "second_img")
</span>        <span class="c1"># label_path = os.path.join(dl_manager._data_dir, "label")
</span>
        <span class="n">metadata_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">dl_manager</span><span class="p">.</span><span class="n">_data_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">metadata.jsonl</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">first_img_path</span> <span class="o">=</span> <span class="n">dl_manager</span><span class="p">.</span><span class="n">_data_dir</span>
        <span class="n">second_img_path</span> <span class="o">=</span> <span class="n">dl_manager</span><span class="p">.</span><span class="n">_data_dir</span>
        <span class="n">label_path</span> <span class="o">=</span> <span class="n">dl_manager</span><span class="p">.</span><span class="n">_data_dir</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="n">datasets</span><span class="p">.</span><span class="nc">SplitGenerator</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="n">datasets</span><span class="p">.</span><span class="n">Split</span><span class="p">.</span><span class="n">TEST</span><span class="p">,</span>
                <span class="c1"># These kwargs will be passed to _generate_examples
</span>                <span class="n">gen_kwargs</span><span class="o">=</span><span class="p">{</span>
                    <span class="sh">"</span><span class="s">metadata_path</span><span class="sh">"</span><span class="p">:</span> <span class="n">metadata_path</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">first_img_path</span><span class="sh">"</span><span class="p">:</span> <span class="n">first_img_path</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">second_img_path</span><span class="sh">"</span><span class="p">:</span> <span class="n">second_img_path</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">label_path</span><span class="sh">"</span><span class="p">:</span> <span class="n">label_path</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">),</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">_generate_examples</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">metadata_path</span><span class="p">,</span> <span class="n">first_img_path</span><span class="p">,</span> <span class="n">second_img_path</span><span class="p">,</span> <span class="n">label_path</span><span class="p">):</span>
        <span class="c1"># print(metadata_path)
</span>        <span class="n">metadata</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_json</span><span class="p">(</span><span class="n">metadata_path</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">metadata</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
            <span class="n">cur_first_img_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">first_img_path</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">first_img</span><span class="sh">"</span><span class="p">])</span>
            <span class="c1"># cur_first_img_path = row["first_img"]
</span>            <span class="n">first_img</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">cur_first_img_path</span><span class="p">).</span><span class="nf">tolist</span><span class="p">())</span>
            <span class="c1"># first_img = open(cur_first_img_path, "rb").read()
</span>
            <span class="n">cur_second_img_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">second_img_path</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">second_img</span><span class="sh">"</span><span class="p">])</span>
            <span class="c1"># cur_second_img_path = row["second_img"]
</span>            <span class="n">second_img</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">cur_second_img_path</span><span class="p">).</span><span class="nf">tolist</span><span class="p">())</span>
            <span class="c1"># second_img = open(cur_second_img_path, "rb").read()
</span>
            <span class="n">cur_label_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">label_path</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">])</span>
            <span class="c1"># cur_label_path = row["label"]
</span>            <span class="n">label</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">cur_label_path</span><span class="p">).</span><span class="nf">item</span><span class="p">()])</span>
            <span class="c1"># label = open(cur_label_path, "rb").read()
</span>
            <span class="c1"># return key and value pair
</span>            <span class="k">yield</span> <span class="n">row_idx</span><span class="p">,</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">first_image</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="c1"># "path": cur_first_img_path,
</span>                    <span class="c1"># "bytes": first_img
</span>                    <span class="n">first_img</span>
                <span class="p">},</span>
                <span class="sh">"</span><span class="s">second_image</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="c1"># "path": cur_second_img_path,
</span>                    <span class="c1"># "bytes": second_img,
</span>                    <span class="n">second_img</span>
                <span class="p">},</span>
                <span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
                    <span class="c1"># "path": cur_label_path,
</span>                    <span class="c1"># "bytes": label
</span>                    <span class="n">label</span>
                <span class="p">}</span>
            <span class="p">}</span>
</code></pre></div></div> <p>In the other two files, change <code class="language-plaintext highlighter-rouge">name=datasets.Split.TEST</code> in <code class="language-plaintext highlighter-rouge">_split_generators</code> function to <code class="language-plaintext highlighter-rouge">name=datasets.Split.TRAIN</code> and <code class="language-plaintext highlighter-rouge">name=datasets.Split.VALIDATION</code>.</p> <p>Some explanation</p> <ul> <li>in <code class="language-plaintext highlighter-rouge">_FEATURES</code> I define all the three to be <code class="language-plaintext highlighter-rouge">datasets.Array2D</code>, simply because there is no datasets.Array1D and idk why …</li> <li>in <code class="language-plaintext highlighter-rouge">_generate_examples</code> each np array is wrapped by <code class="language-plaintext highlighter-rouge">tuple(..tolist())</code>, because HuggingFace expect this function to return a hashable value in each dict value, so neither np.array nor list can be processed, and converting it to tuple is the only way I can think of. Many tutorials use bytes but that seems to work when you set <code class="language-plaintext highlighter-rouge">datasets.Image</code> in <code class="language-plaintext highlighter-rouge">_FEATURES</code>, and that is exactly what I try to avoid</li> </ul> <p>Now the structure should look like this</p> <p>📁 test<br/> ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label1.npy<br/> │ └── … ├── 📄 metadata.jsonl<br/> ├── 📁 second_img │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── … <br/> └── 📄 test.py<br/> 📁 train<br/> ├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label1.npy<br/> │ └── … ├── 📄 metadata.jsonl<br/> ├── 📁 second_img │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── … <br/> └── 📄 train.py<br/> 📁 val</p> <p>├── 📁 first_img<br/> │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── …<br/> ├── 📁 label<br/> │ ├── label0.npy<br/> │ ├── label1.npy<br/> │ └── … ├── 📄 metadata.jsonl<br/> ├── 📁 second_img │ ├── img0.npy<br/> │ ├── img1.npy<br/> │ └── … <br/> └── 📄 val.py</p> <h1 id="upload-the-data">Upload the Data</h1> <p>First some environment settings:</p> <ul> <li><code class="language-plaintext highlighter-rouge">pip install huggingface_hub</code></li> <li><code class="language-plaintext highlighter-rouge">pip install datasets</code></li> </ul> <p>Then follow the instruction here to login:</p> <p>https://huggingface.co/docs/datasets/en/upload_dataset</p> <p>using <code class="language-plaintext highlighter-rouge">huggingface-cli login</code> and your access tokens.</p> <p>Then, suppose all the three folders are in a folder named <code class="language-plaintext highlighter-rouge">paired_mnist</code>. In this <code class="language-plaintext highlighter-rouge">paired_mnist</code> folder run the following script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">DatasetDict</span>

<span class="n">ddict</span> <span class="o">=</span> <span class="nc">DatasetDict</span><span class="p">({</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">:</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">:</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">:</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">]})</span>

<span class="n">ddict</span><span class="p">.</span><span class="nf">push_to_hub</span><span class="p">(</span><span class="sh">"</span><span class="s">stonezh/PairedMNIST</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># &lt;- I am using my online repo id here
</span></code></pre></div></div>]]></content><author><name></name></author><category term="coding"/><category term="HuggingFace"/><category term="Croissant"/><summary type="html"><![CDATA[Why this Matters]]></summary></entry><entry><title type="html">Paper Reviewing - Self-Supervised Learning with Kernel Dependence Maximization</title><link href="https://stonezhng.github.io/blog/2023/PaperReviewingSelf-SupervisedLearningwithKernelDependenceMaximization/" rel="alternate" type="text/html" title="Paper Reviewing - Self-Supervised Learning with Kernel Dependence Maximization"/><published>2023-11-24T16:28:00+00:00</published><updated>2023-11-24T16:28:00+00:00</updated><id>https://stonezhng.github.io/blog/2023/PaperReviewingSelf-SupervisedLearningwithKernelDependenceMaximization</id><content type="html" xml:base="https://stonezhng.github.io/blog/2023/PaperReviewingSelf-SupervisedLearningwithKernelDependenceMaximization/"><![CDATA[<p>https://proceedings.neurips.cc/paper_files/paper/2021/file/83004190b1793d7aa15f8d0d49a13eba-Paper.pdf</p> <h1 id="metric-space">Metric Space</h1> <p>A metric space is a set with a notion of distance defined between every pair of elements in the set, the distance is called metric or distance function. A complete metric space is a metric space where a Cauchy sequence of any point in the metric space is also in the same metric space. Examples: on the real number set interval (0, 1) with absolute difference metric, a sequence $x_n = \frac{1}{n}$, the sequence is Cauchy, but it converges to 0 which is outside of the interval, so the metric space is not complete.</p> <h2 id="hilbert-space">Hilbert Space</h2> <p>A Hilbert space is a <a href="https://en.wikipedia.org/wiki/Vector_space" title="Vector space">vector space</a> equipped with an <a href="https://en.wikipedia.org/wiki/Inner_product" title="Inner product">inner product</a> that induces a <a href="https://en.wikipedia.org/wiki/Distance_function">distance function</a> for which the space is a <a href="https://en.wikipedia.org/wiki/Complete_metric_space" title="Complete metric space">complete metric space</a>.</p> <table> <tbody> <tr> <td>Hilbert space adds an additional constraint on how the distance function is defined on a complete metric space. For example, on $\mathbb{R}^2$ with inner product $(x_1, x_2) \cdot (y_1, y_2) = x_1y_1 + x_2y_2$ , a norm $</td> <td> </td> <td>x</td> <td> </td> <td>$ is defined by $</td> <td> </td> <td>x</td> <td> </td> <td>= \sqrt{x \cdot x}$, and the distance between two points is defined b $</td> <td> </td> <td>x - y</td> <td> </td> <td>$, which is just the common Euclidean distance on $\mathbb{R}^2$, but constructed from perspective of inner product.</td> </tr> </tbody> </table> <h2 id="reproducing-hilbert-kernel-space">Reproducing Hilbert Kernel Space</h2> <p>https://www.youtube.com/watch?v=EoM_DF3VAO8 https://www.youtube.com/watch?v=ABOEE3ThPGQ</p> <h3 id="kernel">Kernel</h3> <p>The idea of kernel is to connecting between linear space and high dimensional space. Suppose we have a random space $\mathcal{X}$ , and we have a function $\phi: \mathcal{X} \rightarrow \mathcal{H}$ that maps a point in $\mathcal{X}$ to a point in another space $\mathcal{H}$. The new space $\mathcal{H}$ is a Hilbert space which guarantees some good properties, and a typical example of a Hilbert space is $\mathbb{R}^d$ (d-dimensional Euclidean space). In machine learning, the intuition of seeking such a mapping function $\phi$ is that the classification algorithms such as SVM are easier to optimized on $\mathbb{R}^d$, which means with $\phi$ we can run the algorithms as efficiently as on $\mathbb{R}^d$ with data samples collected from more complex spaces.</p> <p>Because $\phi(x) \in \mathcal{H}$, it is natural that we look at the inner product between $\phi(x)$ and $\phi(y)$, i.e. $\phi(x) \cdot \phi(y)$. We hope there is a direct way to get this inner product scalar value from $\mathcal{X}$ instead of computing $\phi$, so we propose a new function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, such that $k(x, y) = \phi(x) \cdot \phi(y)$. This approach to compute inner product directly is called kernel trick, and $k$ is called a kernel function.</p> <p>A kernel function always satisfies the following properties:</p> <ul> <li>Symmetry. $k(x, y) = k(y, x)$. Since the inner product on $\mathcal{H}$ is symmetric.</li> <li>Has a corresponding positive semi-definite kernel metric. We construct a kernel metric $K$ on any $n$ data points $x_1, x_2, \cdots, x_n \in \mathcal{X}$ by setting $K_{i, j} = k(x_i, x_j)$. $K$ is positive semi-definite, which means that $\forall c \in \mathbb{R}^n$, $c^TKc \ge 0$</li> </ul> <h3 id="constructing-reproducing-kernel-hilbert-space">Constructing Reproducing Kernel Hilbert Space</h3> <p>Reproducing Kernel Hilbert Space answers the following question : given a set $\mathcal{X}$ and a kernel function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, how do we construct a Hilbert space $\mathcal{H}$ and the mapping function $\phi$ ? Basically this is an inverse version of how a kernel function is constructed, i.e. how we find the two component (a Hilbert space and a mapping function) that can give us the known $k$ .</p> <p>We start by define the following function</p> \[k_x(\cdot) = k(x, \cdot)\] <table> <tbody> <tr> <td>Using $k_x(\cdot)$, we map $x \in \mathcal{X}$ to an image of a function $k_x$. We then construct a spamming set $\mathcal{G}$ from ${k_x</td> <td>x \in \mathcal{X} }$ by taking all $k_x$ as elements and compute all of the finite linear combination of them:</td> </tr> </tbody> </table> \[\mathcal{G} = \{\sum_{i=1}^r m_i k_{x_i} | m_i \in \mathbb{R}, r \in \mathbb{N} \}\] <p>$\mathcal{G}$ is quite close to the Hilbert space we are looking for, but we need to do two things:</p> <ul> <li>Define the inner product. The way we define the inner product is</li> </ul> <p>\(k_{x_i} \cdot k_{x_j} = k(x_i, x_j)\) for $x_i \in \mathcal{X}$ and $x_i \in \mathcal{X}$</p> \[\sum_{i=1}^r \alpha_i k_{x_i} \cdot \sum_{j=1}^l \beta_j k_{y_j} = \sum_{i, j} \alpha_i\beta_jk(x_i, y_j)\] <p>Assuming that all $x_i \in \mathcal{X}$ and all $y_j \in \mathcal{X}$</p> <ul> <li>Add all the complement to make it a complete space, i.e. add all limits of Cauchy sequences.</li> </ul> <p>We now call this new $\bar{\mathcal{G}}$ a Reproducing Kernel Hilbert Space $\mathcal{H}$, and by constructing it satisfies $k(x, y) = \phi(x) \cdot \phi(y)$</p> <p>One key property of RHKS is the <strong>reproducing property</strong>:</p> <p>Let $f = \sum_{i=1}^r m_i k_{x_i}$, i.e. an element in RHKS, then $f \cdot k_x = f(x)$ . Notice the difference between $f$ and $f(x)$ here, the LHS is the inner product on two elements in RHKS, i.e. two images, while the RHS is a function value “reproducing” one of the function on LHS.</p> <h1 id="issues-of-infonce-in-self-supervised-learning">Issues of InfoNCE in self supervised learning</h1> <p>https://lilianweng.github.io/posts/2021-05-31-contrastive/</p> <p>InfoNCE performance cannot be explained solely by the properties of the mutual information, but is influenced more by other factors, such as the formulation of the estimator and the architecture of the feature extractor. Essentially, representations with the same MI can have drastically different representational qualities.</p> <p>The example in the paper:</p> <table> <tbody> <tr> <td>Suppose we have two inputs $a$ and $b$, and an encoder $E(\cdot</td> <td>M)$ parameterized by an integer $M$ that maps $a$ uniformly randomly to ${0, 2, \cdots, 2M}$ and $b$ uniformly randomly to ${1, 3, \cdots, 2M+1}$. Let us denote $z_a = E(a</td> <td>M)$ and $z_b = E(b</td> <td>M)$,</td> </tr> </tbody> </table> \[\text{MI}(z_a; z_b) = \sum_{z_a, z_b}p(z_a, z_b)\log \frac{p(z_a, z_b)}{p(z_a)p(z_b)}\] <p>For the joint distribution, $p(z_a=m_a, z_b=m_b) = \frac{1}{(M+1)^2}$</p> <h1 id="hilbert-schmidt-independence-criterion-hsic">Hilbert-Schmidt Independence Criterion (HSIC)</h1> <p>Suppose we have two input space $\mathcal{X}$ and $\mathcal{Y}$ , and we have two mapping functions $\phi: \mathcal{X} \rightarrow \mathcal{F}$ and $\psi: \mathcal{Y} \rightarrow \mathcal{G}$ , $\mathcal{F}$ and $\mathcal{G}$ being two reproducing kernel Hilbert spaces. HSIC is defined as:</p> \[\text{HSIC}(X; Y) = ||\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]||^2_{\text{HS}}\] <table> <tbody> <tr> <td>Which is the norm of the covariance between the mapped elements. The Hilbert-Schmidt norm $</td> <td> </td> <td>\cdot</td> <td> </td> <td>_{\text{HS}}$ in finite dimensions is the usual Frobenius norm ( the square root of the sum of the squares of its elements).</td> </tr> </tbody> </table> <p>Remember that the mapping function $\phi(X)$ and the corresponding inner product on RKHS can be represented by the kernel function, so by expanding the expression of HSIC we get:</p> \[\begin{align} \text{HSIC}(X; Y) &amp;= ||\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]||^2_{\text{HS}} \\ &amp;= &lt;(\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]), (\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)])&gt;_{\text{HS}} \\ &amp;= &lt;\mathbb{E}[\phi(X) \cdot \psi(Y)], \mathbb{E}[\phi(X) \cdot \psi(Y)]&gt;_{\text{HS}} \\ &amp; - 2&lt;\mathbb{E}[\phi(X) \cdot \psi(Y)], \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]&gt;_\text{HS} \\ &amp; + &lt;\mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)], \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]&gt;_\text{HS} \\ &amp;= \mathbb{E}&lt;[\phi(X) \cdot \psi(Y)], [\phi(X') \cdot \psi(Y')]&gt;_{\text{HS}} \\ &amp; - 2\mathbb{E}&lt;[\phi(X) \cdot \psi(Y)], [\phi(X')] \cdot \mathbb{E}[\psi(Y'')]&gt;_\text{HS} \\ &amp; + &lt;\mathbb{E}[\phi(X) \cdot \phi(X')], \mathbb{E}[\psi(Y) \cdot \psi(Y')]&gt;_\text{HS} \\ &amp;= \mathbb{E}[k(X, X')l(Y, Y')] - 2\mathbb{E}[k(X, X')l(Y, Y'')] + \mathbb{E}[k(X, X')]\mathbb{E}[l(Y, Y')]] \end{align}\] <p>assuming that $k$ and $l$ are two kernel functions on the two RKHS.</p> <p>Given $N$ iid drawn samples ${(X_1, Y_1), (X_2, Y_2), \cdots, (X_N, Y_N)}$, an empirical estimator is proposed by Gretton et al.</p> \[\widehat{\text{HSIC}}(X; Y) = \frac{1}{(N-1)^2}\text{Tr}(KHLH)\] <p>where $K$ and $L$ are the kernel matrices of $k$ and $l$ respectively on the $N$ data points from $\mathcal{X}$ and $\mathcal{Y}$, and $H = I - \frac{1}{N}11^T$ is called the centering matrix.</p>]]></content><author><name></name></author><category term="math"/><category term="math,"/><category term="metric,"/><category term="deep"/><category term="learning"/><summary type="html"><![CDATA[https://proceedings.neurips.cc/paper_files/paper/2021/file/83004190b1793d7aa15f8d0d49a13eba-Paper.pdf]]></summary></entry><entry><title type="html">Reviewing the paper Longitudinal data analysis using matrix completion</title><link href="https://stonezhng.github.io/blog/2023/SequentialLMM/" rel="alternate" type="text/html" title="Reviewing the paper Longitudinal data analysis using matrix completion"/><published>2023-10-16T21:44:00+00:00</published><updated>2023-10-16T21:44:00+00:00</updated><id>https://stonezhng.github.io/blog/2023/SequentialLMM</id><content type="html" xml:base="https://stonezhng.github.io/blog/2023/SequentialLMM/"><![CDATA[<p>Let $\mathbf{b}(t) = (b_1(t), b_2(t), \cdots, b_K(t))$ be a vector of $K$ basis elements at timepoint $t$. It is considered a truncated basis because the assumption of $K$ elements may not sufficiently cover the full vector space.</p> <h1 id="sequential-lmm">Sequential LMM</h1> <p>The general goal is to get a mapping from time $\mathbf{t}_i$ to target space $\mathbf{y}_i$ for every observed sample $i \in {1, 2, \cdots, N}$.</p> <p>The paper called a mean at time t, i.e. $\mu(t)$ a <em>fixed effect</em>. $\mu(t)$ is defined as a weighted summary on the truncated vector space $\mathbf{b}(t)$ of length $K$, i.e.</p> \[\mu(t) = \mathbf{m} \cdot \mathbf{b}(t)\] <p>where $\mathbf{m} = (m_1, m_2, \cdots, m_K)$, $\mathbf{b}(t) = (b_1(t), b_2(t), \cdots, b_K(t))$</p> <p>The paper then introduces a per-sample coefficient , aka the individual <em>random effect</em>, $\mathbf{w}$. $\mathbf{w}$ can be an $N \times K$ matrix, where each row is for a data sample. For a sample’s coefficient $\mathbf{w}<em>i, i\in {1, 2, \cdots, N}$ of dim $K$, we assume that there are $n_i$ time steps for this sample, i.e. the time sequence looks like ${t</em>{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ of length $n_i$, and at every time step $t_{i, j}$ there is a fixed effect $\mu(t_{i, j})$ , leading to a $n_i$ - dim vector of $(\mu(t_{i, 1}), \mu(t_{i, 2}) \cdots, \mu(t_{i, n_i}))$ as the fixed effect for over all this individual’s time sequence. We denote this fixed effect vector of dim $n_i$ over this $i-$th individual sample as $\boldsymbol{\mathbf{\mu}}<em>i$. For each scalar $\mu(t</em>{i, j})$, it follows the same definition above and takes the truncated basic vector at time $t_{i,j}$, i.e. $\mu(t_{i, j}) = \mathbf{m} \cdot \mathbf{b}(t_{i, j})$ .</p> <p>To model the target distribution $\mathbf{y}_i$ from the observed individual $\mathbf{w}_i$,. we follow the traditional LMM approach via a conditional gaussian:</p> \[\mathbf{y}_i|\mathbf{w}_i \sim \mathcal{N}(\boldsymbol{\mathbf{\mu}}_i + \mathbf{B}_i \cdot \mathbf{w}_i, \sigma^2 \mathbf{I}_{n_i})\] <p>assuming that the target distribution $\mathbf{y}<em>i$ is a sequence of scalar, each scalar value representing the state at $t</em>{i, j}$, $j \in {1, 2, \cdots, n_i}$. Also, $\mathbf{B}<em>i$ is naturally a matrix of dimension $(n_i, K)$ to map the $K$-dim per-sample coefficient $\mathbf{w}_i$ to the same time sequence space. Intuitively, $\mathbf{B}_i = [\mathbf{b}(t</em>{i, 1}), \mathbf{b}(t_{i, 2}), \cdots, \mathbf{b}(t_{i, n_i})]$</p> <h1 id="low-rank-sequential-lmm">Low Rank Sequential LMM</h1> <p>We restrict the latent space to $q &lt; K$ dimensions and learn from the data the mapping $\mathbf{A} \in \mathbb{R}^{K \times q}$ in this reduced dimension LMM:</p> <p>\(\mathbf{y}_i|\mathbf{w}_i \sim \mathcal{N}(\boldsymbol{\mathbf{\mu}}_i + \mathbf{B}_i \cdot A \cdot \mathbf{w}_i, \sigma^2 \mathbf{I}_{n_i})\) where now the individual sample $\mathbf{w}_i$ also reduces its dim to $q$ . Because the optimizations are on $\mathbf{w}_i$ and $\mathbf{A}$ , an EM algorithm can be applied.</p> <h1 id="this-paper">This paper</h1> <p><a href="https://www.researchgate.net/publication/327858691_Longitudinal_data_analysis_using_matrix_completion">original paper</a></p> <p>Instead of using existing ${t_{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ for every sample $i$, the paper defined a universal grid agnostic of actual time series per sample:</p> <p>\(G = [\tau_1, \tau_2, \cdots, \tau_T]\) where $\tau_1$ is the global minimal time across all sample $i$, $\tau_T$ is the global maximal time across all sample $i$, and $T$ is a hyperparameter defining how accurate the grid is. Ideally if $T$ is infinity the grid becomes a continuous space that can capture every $t_{i, j}$.</p> <p>For every $t_{i, j}$ , we locate its corresponding</p> \[g_i(j) = \text{argmin}_{1 \le k \le T}(|\tau_k - t_{i, j}|)\] <p>i.e. the index of $\tau$ that is most close to $t_{i, j}$. We also define $\tilde{\mathbf{y}}<em>{i, j} = \mathbf{y}</em>{i, g_i(j)}$.</p> <p>For sample $i$, now we can map the original time sequence ${t_{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ to ${g_i(1), g_i(2), \cdots, g_i(n_i)}$, and the paper called this new sequence of $g_i$ as $O_i$. For sample $i$, any index in $[1, T]$ that not in $O_i$ is marked as a missing target sample, which is quite natural because $G$ is designed to fit all samples so will always greater than or equal to the time sequence of individual sample. The paper introduces an additional notation $\Omega$ to represent $O_i$ over all samples, i.e. $\Omega = {(i, j): 1 \le i \le N, j \in O_i }$. Finally, the paper reformulated the label space to a $N \times T$ matrix $P_\Omega(\mathbf{Y})$ where:</p> \[P_\Omega(\mathbf{Y})[i, j] = \mathbf{y}_i[O_{i}[j]]\] <p>if target is not missing, 0 otherwise. The operator $P_\Omega(\cdot)$ is a general projection that projects input from space $\Omega$ to the full grid space, by keeping the original value if the input has a corresponding value at that grid position, otherwise fill the value at that grid position with 0.</p> <p>The paper still used the same $\mathbf{b}(t)$ of dim $K$, but $\mathbf{B}_i$ is now replaced by a sample-agnostic $\mathbf{B} = [\mathbf{b}(\tau_1), \mathbf{b}(\tau_2), \cdots, \mathbf{b}(\tau_T)]$ .</p> <p>Again following the same approach we want to minimize the difference between every $\tilde{\mathbf{y}}<em>{i, j}$ and every predicted $\mathbf{w}_i \cdot \mathbf{b}(\tau</em>{g_i(j)})$, i.e.</p> <p>\(\text{argmin}_{\mathbf{w}_i} |\tilde{\mathbf{y}}_{i, j} - \mathbf{w}_i \cdot \mathbf{b}(\tau_{g_i(j)})|\) for all $i$, which is equivalent to the matrix form of:</p> \[\text{argmin}_{\mathbf{W}}||P_\Omega(\mathbf{Y} - \mathbf{W}\cdot \mathbf{B}^T)||_F\] <table> <tbody> <tr> <td>where $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \cdots, \mathbf{w}_N]$ of dim $(N \times K)$, $\mathbf{B} = [\mathbf{b}(\tau_1), \mathbf{b}(\tau_2), \cdots, \mathbf{b}(\tau_T)]$ of dim $(T, K)$ , and $</td> <td> </td> <td>\cdot</td> <td> </td> <td><em>F$ being Frobenius norm, i.e. the square root of the sum of matrix elements. Again projection operator $P</em>\Omega$ guarantees that we only keep all available $\mathbf{y}_i$ and skip all missing y in the full grid.</td> </tr> </tbody> </table> <h2 id="soft-longitudinal-impute">Soft-Longitudinal-Impute</h2> <p>To optimize the above loss with missing ys, the paper utilized the fact that optimization of</p> \[\text{argmin}_\mathbf{W} (\frac{1}{2} ||\mathbf{Y} - \mathbf{W} \cdot \mathbf{B}^T||_F^2 + ||\mathbf{W}||_*)\] <table> <tbody> <tr> <td>has a unique solution $\mathbf{W} = S_\lambda(\mathbf{Y}\mathbf{B})$, where $S_\lambda(X) = UD_\lambda V$ and $X = UDV^T$ is the SVD of $X$. Notation: $</td> <td> </td> <td>\cdot</td> <td> </td> <td><em>*$ is the nuclear norm, i.e. the sum of singular values. $D</em>\lambda$ is a diagonal matrix called soft-thresholding, which does a threshold on the original diagonal matrix $D$ by $D_\lambda[i, i] = \text{max}(D[i, i]-\lambda, 0)$, i.e. subtract $\lambda$ from every diagonal value, keep the subtraction value if it is greater than 0 otherwise set it to 0.</td> </tr> </tbody> </table> <p>Intuitively, this algorithm approaches $\mathbf{W}$ iteratively by a new Y with imputed values to fill in the missing places. For a given approximation of the solution $\mathbf{W}^{\text{old}}$ , we use $\mathbf{W}^{\text{old}}$ to impute unknown elements of $\mathbf{Y}$ obtaining $\tilde{\mathbf{Y}}$ . Then, we construct the next approximation $\mathbf{W}^{\text{new}}$ by running the above SVD based solution on</p> \[\mathbf{W}^{\text{new}} = \text{argmin}_\mathbf{W} (\frac{1}{2} ||\tilde{\mathbf{Y}} - \mathbf{W} \cdot \mathbf{B}^T||_F^2 + ||\mathbf{W}||_*)\] <p>So there wouldn’t be any projection operator.</p> <p>Also because the optimization is sensitive to $\lambda$, the paper proposed that run the optimization with decreasing $\lambda$ to refine the optimization results.</p>]]></content><author><name></name></author><category term="math"/><category term="math,"/><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[Let $\mathbf{b}(t) = (b_1(t), b_2(t), \cdots, b_K(t))$ be a vector of $K$ basis elements at timepoint $t$. It is considered a truncated basis because the assumption of $K$ elements may not sufficiently cover the full vector space.]]></summary></entry><entry><title type="html">Introduction to Normalizing Flows</title><link href="https://stonezhng.github.io/blog/2023/flows/" rel="alternate" type="text/html" title="Introduction to Normalizing Flows"/><published>2023-10-07T16:40:16+00:00</published><updated>2023-10-07T16:40:16+00:00</updated><id>https://stonezhng.github.io/blog/2023/flows</id><content type="html" xml:base="https://stonezhng.github.io/blog/2023/flows/"><![CDATA[<h1 id="basic-normalizing-flow">Basic Normalizing Flow</h1> <p>A <strong>normalizing flow</strong> is similar to a VAE in that we try to build up $p(X)$ by starting from a simple known distribution $p(Z)$. $X$ and $Z$ are of <strong>same dimensionality</strong>.</p> <p>The very basic idea of a flow connecting a complex $X$ and a simple $Z$ would be using a <strong>bijective</strong> function $f$ with its inverse $f^{-1}$ such that:</p> <ul> <li>$X = f(Z)$</li> <li>$Z = f^{-1}(X)$</li> <li>$p(X) = p_Z(f^{-1}(X))|\text{delta}(\mathbf{J}_{f^{-1}})|$ where the term on the right is the absolute value of the determinant of the Jacobian of $f^{-1}(X)$ w.r.t. $X$, and $p_Z$ uses a subscription $Z$ to differentiate it from the distribution of $X$.</li> </ul> <p>It is natural to introduce a chain structure between $X$ and $Z$:</p> <ul> <li>$X = f_1(f_0(Z))$</li> <li>$Z = f_0^{-1}(f_1^{-1}(X))$</li> <li>$p(X) = p_Z(f_0^{-1}(f_1^{-1}(X))) \left\lvert \text{delta}(\mathbf{J}<em>{f_0^{-1}}) \right\lvert \left\lvert \text{delta}(\mathbf{J}</em>{f_1^{-1}}) \left\right$</li> </ul> <h2 id="training-process">Training Process</h2> <p>Since most of the time $X$ is what we observed and has an empirical distribution consisting of $N$ data points ${X_1, X_2, \cdots, X_N}$, we can simply optimize the negative loglikelihood:</p> \[\text{argmin}_f \sum_{i = 1}^N -\log (p(X_I)) = \text{argmin}_f \sum_{i = 1}^N -\log (p_Z(f^{-1}(X_i))|\text{delta}(\mathbf{J}_{f^{-1}(X_i)})|)\] <h2 id="commonly-used-flows">Commonly Used Flows</h2> <h4 id="planar-flow">Planar Flow</h4> \[X=f(Z)= Z+ \mathbf{u} h(\mathbf{w}^TZ+b)\] <p>$\mathbf{u}, \mathbf{w}$ and $b$ are parameters. Additional constraints are required on $\mathbf{u}, \mathbf{w}, b$ and $h$ to guarantee the function is bijective. For example, it is ituitive that $h$ needs to be bijective, like $h = \tanh$.</p> <p>The Jabobian of $f^{-1}(X)$ is not obvious, as it depends on $h$, so the analytic form of $p(X)$ is not easy to compute.</p> <h3 id="nonlinear-independent-components-estimation-nice">Nonlinear Independent Components Estimation (NICE)</h3> <p>This method requires that $X$ can be split into two disjoint parts, $X_1$ and $X_2$, i.e. $X = [X_1, X_2]$. Same assumption for $Z$.</p> <p>Forward mapping $f: Z \rightarrow X$</p> <ul> <li>$X_1 = Z_1$</li> <li>$X_2 = Z_2 + m_\theta(Z_1)$, where $m_\theta$ is a neural network Inverse mapping $f^{-1}: X \rightarrow Z$</li> <li>$Z_1 = X_1$</li> <li>$Z_2 = X_2 - m_\theta(X_1)$ The inverse mapping of $Z_2$ can be simply obtained by replacing $Z_1$ in forward mapping of $X_2$ with $X_1$ and move $m_\theta(Z_1)$ (equivalent to $m_\theta(X_1)$) to LHS.</li> </ul> <p>The Jacobian of the forward mapping is lower trangular, whose determinant is simply the product of the elements on the diagonal, i.e.</p> \[\frac{\partial f^{-1}(X)}{\partial X} = \begin{pmatrix} \frac{\partial (Z_1)}{\partial X_1} &amp; \frac{\partial (Z_1)}{\partial X_2} \\ \frac{\partial (Z_2)}{\partial X_1} &amp; \frac{\partial (Z_2)}{\partial X_2} \end{pmatrix} =\begin{pmatrix} I &amp; 0 \\ \frac{\partial (-m_\theta(X_1))}{\partial X_1} &amp; I \end{pmatrix}\] \[\text{delta}(\begin{pmatrix} I &amp; 0 \\ \frac{\partial (-m_\theta(X_1))}{\partial X_1} &amp; I \end{pmatrix}) = 1\] <p>Therefore, this defines a volume preserving transformation.</p> <h1 id="continuous-normalizing-flows">Continuous Normalizing Flows</h1> <p><a href="https://browse.arxiv.org/pdf/1806.07366.pdf">Original paper</a></p> <p>Continuous Normalizing Flows solves the problem of selecting proper transition functions and the high computation complexity of Jacobians. The idea first origniates from a discrete sequence based transition:</p> \[X_{t+1} = X_t + f(X_t, \theta_t)\] <p>where $f$ is time-sensitive with the parameter $\theta_t$ changing with time. From a perspective of flows, this sequence transition provides a natural way of connecting $X$ and $Z$, i.e. $X$ being $X_T$ at the end of the sequence and $Z$ being $X_0$ at the beginning of the sequence, with finite discrete intermediate states $X_t, t \in [1, T-1] \cap \mathbb{N}^+$. The transition is obviously bijective, since each step is a simple linear function.</p> <p>We then want to see what if the sequence becomes continous. Since the discrete sequence transition is describing the change at time $t$ in a discrete manor, i.e. $X_{t+1} - X_t = \Delta X_{t} =f(X_t, \theta_t)$, the continous version is then: \(\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)\) which is an ordinary differential equation (ODE). Starting from the input layer$X_0$ ($Z$), we can define the output layer$X_T$ to be the solution to this ODE initial value problem at some time $T$ and emprically computed by some black box ODE solver:</p> \[X_t = X_0 + \int_0^tf(X_i, i, \theta)di = \text{ODESolve}(X_0, f, 0, t, θ)\] <h2 id="computation-of-jacobian">Computation of Jacobian</h2> <p>With the ODE to define the continous mapping from $X_t$ to $X_{t+1}$, the next question is how the probablity would change from $t$ to $t+1$. In the discrete case, we have</p> <table> <tbody> <tr> <td>$$\log (p(X_{t+1})) - \log(p(X_t)) = \log(</td> <td>\text{delta}(\mathbf{J}_{F_t^{-1}})</td> <td>)$$ ,</td> </tr> </tbody> </table> <p>where $F_t$ is the mapping function at time $t$: $X_{t+1} = X_{t} + \int_t^{t+1}f(X_i, i, \theta)di = \text{ODESolve}(X_t, f, t, t+1, θ)$.</p> <p>The paper proved that in the continous case,</p> \[\frac{\partial \log(p(X_t))}{\partial t} = -\text{tr}(\frac{\partial f}{\partial X_t})\] <p>with same $f$ defined in the ODE $\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)$. ($f$ is of the same dimensionality as $X_t$ so its first derivative w.r.t $X_t$ is a square matrix, and this theorem says we can only compute the diagnal elements and sum them up)</p> <p>An example made by the author is a continous version of Planar Flow, where the function $\mathbf{u}h(\cdot)$ is not served as a direct mapping from $X_t$ to $X_{t+1}$, but describing the gradient (dynamic) at $X_t$:</p> \[\frac{\partial X_t}{\partial t} = \mathbf{u}h(\mathbf{w}^TX_t + b)\] \[\frac{\partial \log(p(X_t))}{\partial t} = -\mathbf{u}^T\frac{\partial h}{\partial X_t}\] <p>The two ODEs then give way of sampling complex $X$ ($X_t$)from simple random variable $Z$ ($X_0$) by the first ODE and estimate its density by the second ODE, assuming we have a good black box ODE solver.</p> <p>The author did two addtional things to the two ODEs.</p> <ul> <li>Because the trace function is linear, we can easily add multiple $f$s to ODE:</li> </ul> \[\frac{\partial X_t}{\partial t} = \sum_{i=1}^N f_i(X_t, t, \theta)\] \[\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\frac{\partial f_i}{\partial X_t})\] <ul> <li>Also by utilizing the linearity, we can specify the role of $t$ in $f$ in a gating mechanism manner:</li> </ul> \[\frac{\partial X_t}{\partial t} = \sum_{i=1}^N \sigma_i(t)f_i(X_t, \theta)\] \[\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\sigma_i(t)\frac{\partial f_i}{\partial X_t})\] <p>where $f_i$ now is independent of $t$</p> <h2 id="backpropogation">Backpropogation</h2> <p>A vanilla way of computing the gradients of all $f$ is expensive. The author proposed using the adjoint sensitivity method, which computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers.</p> <p>Suppose we have a scalar loss function $L$ on the output $X_{t_1} = \text{ODESolve}(X_{t_0}, f, t_0, t_1, θ)$, written as \(L(\text{ODESolve}(X_{t_0}, f, t_0, t_1, θ))\)The target is $\frac{\partial L}{\partial \theta}$.</p> <p>We first work on the adjoint \(\mathbf{a}(t) = \frac{\partial L}{\partial X_{t}}\)Its gradients (dynamics) are given by another ODE, which can be thought of as the instantaneous analog of the chain rule:</p> \[\frac{\partial \mathbf{a}(t)}{\partial t} = -\mathbf{a(t)}^T\frac{\partial f(X_t, t, \theta)}{\partial X_{t}}\] <p>We can then compute $\mathbf{a}(t)$ by another call to an ODE solver. This solver <strong>must run backwards,</strong> starting from the initial value of $\mathbf{a}(t_1)$. Because we need to know $X_t$ when computing this gradient at time $t$, we need to reversely get $X_t$ starting from $t_1$ together with the backward computation of $\mathbf{a}(t)$.</p> <p>Finally we use $\mathbf{a}(t)$ to compute $\frac{\partial L}{\partial \theta}$:</p> \[\frac{\partial L}{\partial \theta} = -\int_{t_0}^{t_1}\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta} dt\] <p>which is another ODE:</p> \[\frac{\partial \frac{\partial L}{\partial \theta}}{\partial t} = -\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta}\] <p>so call ODESolver again.</p> <h1 id="riemannian-continuous-normalizing-flows">Riemannian Continuous Normalizing Flows</h1> <p><a href="https://proceedings.neurips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf">Original paper</a></p> <p>This work heavliy relies on the understanding of topology so I add an additional background section.</p> <p>In this work, flows are defined via vector fields on manifolds and computed as the solution to the associated ordinary differential equation (ODE). Intuitively, this method operates by first parametrizing a vector field on the manifold with a neural network, then sampling particles from a base distribution, and finally approximating their flow along the vector field using a numerical solver. The high level idea is, for the ODE of the dynamic: \(\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)\) We are now considering $f(X_t, t, \theta)$ as a vector field, and it is describing the velocity at $X_t$. Consider the temporal evolution of a particle $X_t$ on a $d$-dimensional manifold $\mathcal{M}$, whose velocity is given by a vector field $f(X_t, t, \theta)$. Intuitively,$f(X_t, t, \theta)$) indicates the direction and speed along which the particle is moving on the manifold’s surface. Classic examples for such vector fields include weathercocks giving wind direction and compasses pointing toward the magnetic north pole of the earth.</p> <h2 id="geometric-backgrounds">Geometric Backgrounds</h2> <p>Mostly based on <a href="https://www.cis.upenn.edu/~cis6100/cis61005sl7.pdf">this</a></p> <h3 id="mainfold">Mainfold</h3> <p>It is a topological space that can be covered by a collection of open subsets, $\mathbb{U}<em>\alpha$, where each $\mathbb{U}</em>\alpha$ is isomorphic to some “standard model”, e.g., some open subset of Euclidean space, $\mathbb{R}^n$. (an isomorphism is a structure-preserving mapping between two structures of the same type that can be reversed by an inverse mapping )</p> <h3 id="chart">Chart</h3> <p>Given a topological space, $\mathcal{M}$, a chart (or <strong>local coordinate map</strong>) is a pair, $(\mathbb{U}, \varphi)$, where $\mathbb{U}$ is an open subset of $\mathcal{M}$ and $\varphi: \mathbb{U} \rightarrow \mathbb{\Omega}$ is a homeomorphism onto an open subset, $\mathbb{\Omega} = \varphi(\mathbb{U})$, of $\mathbb{R}^{n_\phi}$ (for some $n_\phi \ge 1$).</p> <p>( <strong>homeomorphism</strong> (from <a href="https://en.wikipedia.org/wiki/Greek_language" title="Greek language">Greek</a> <a href="https://en.wiktionary.org/wiki/%E1%BD%85%CE%BC%CE%BF%CE%B9%CE%BF%CF%82" title="wikt:ὅμοιος">ὅμοιος</a> <em>(homoios)</em> ’similar, same’, and <a href="https://en.wiktionary.org/wiki/%CE%BC%CE%BF%CF%81%CF%86%CE%AE" title="wikt:μορφή">μορφή</a> <em>(morphē)</em> ’shape, form’, named by Henri Poincaré, also called <strong>topological isomorphism</strong>, or <strong>bicontinuous function</strong>, is a bijective and continuous function between topological spaces that has a continuous inverse function.)</p> <p>A more ituitive way of describing a chart is found <a href="https://en.wikipedia.org/wiki/Atlas_(topology)">here</a>: A <strong>chart</strong> for a topological space $\mathcal{M}$ (also called a <strong>coordinate chart</strong>, <strong>coordinate patch</strong>, <strong>coordinate map</strong>, or <strong>local frame</strong>) is a homeomorphism  $\varphi$  from an open subset $\mathbb{U}$ of $\mathcal{M}$ to an open subset of a <strong>Euclidean space</strong>. The chart is traditionally recorded as the ordered pair $(\mathbb{U}, \varphi)$.</p> <p>For a point $p \in \mathbb{U}$ , we obtain its <strong>local coordinates</strong> by first mapping $p$ to Euclidean space via $\varphi$, i.e. $\varphi(p)$, then you do projection to $n_\varphi$ coordinates in this Euclidean space. The coordinates can be written as $(\text{Proj}(\varphi(p))<em>1, \text{Proj}(\varphi(p))_2, \cdots, \text{Proj}(\varphi(p))</em>{n_\varphi})$</p> <h3 id="atlas">Atlas</h3> <p>Not that god in Greek myth.</p> <p>An atlas of $\mathcal{M}$ consists of individual charts that, roughly speaking, describe individual regions of the manifold. Formally, it is an index family ${(\mathbb{U}<em>\alpha, \varphi</em>\alpha): \alpha \in \text{Some set} }$ which covers $\mathcal{M}$, i.e. $\cup_\alpha \mathbb{U}_\alpha = \mathcal{M}$.</p> <h3 id="transition-map">Transition Map</h3> <p>A transition map provides a way of comparing two charts of an atlas. To make this comparison, we consider the composition of one chart with the inverse of the other. Suppose $(\mathbb{U}<em>\alpha, \varphi</em>\alpha)$ and $(\mathbb{U}<em>\beta, \varphi</em>\beta)$ are two charts for $\mathcal{M}$ such that $\mathbb{U}<em>\alpha \cap \mathbb{U}</em>\beta$ is not empty. The transition map $\tau_{\alpha, \beta}: \varphi_\alpha(\mathbb{U}<em>\alpha \cap \mathbb{U}</em>\beta) \rightarrow \varphi_\beta(\mathbb{U}<em>\alpha \cap \mathbb{U}</em>\beta)$ (map the $\varphi_\alpha$ based Euclidean of the joint part to $\varphi_\beta$ based Euclidean of the joint part) is defined as: $\tau_{\alpha, \beta} = \varphi_\beta \circ \varphi_\alpha^{-1}$. With $\tau_{\alpha, \beta}$ you can directly jump from $\varphi_\alpha$ based Euclidean to $\varphi_\beta$ based Euclidean on the joint area of the two charts. Note that we can define in the same way a jump from $\varphi_\beta$ to $\varphi_\alpha$ as $\tau_{\beta, \alpha}: \varphi_\beta(\mathbb{U}<em>\alpha \cap \mathbb{U}</em>\beta) \rightarrow \varphi_\alpha(\mathbb{U}<em>\alpha \cap \mathbb{U}</em>\beta)$, $\tau_{\beta, \alpha} = \varphi_\alpha \circ \varphi_\beta ^{-1}$, so the transition map always come in pairs.</p> <h3 id="ck-n-atlas">$C^k$ n-atlas</h3> <p>A family of charts that</p> <ul> <li>Covers the whole mainfold $\mathcal{M}$, $\cup_\alpha \mathbb{U}_\alpha = \mathcal{M}$</li> <li>Every chart projects to Euclidean of the same dimension $n$, $\varphi_i(p_i) \in \mathbb{R}^n \forall i$</li> <li>Whenever $\mathbb{U}<em>i \cap \mathbb{U}_j \ne \Phi$, the transition map $\tau</em>{i, j}$ is a $C^k$ -diffeomorphism (k times differentiable)</li> </ul> <h3 id="compatible">Compatible</h3> <p>Chart $(\mathbb{U}, \varphi)$ is compatible with $C^k$ n-atlas $\mathcal{A}$ iff for every $\mathbb{U}_i \in \text{Charts of }\mathcal{A}$ and $\mathbb{U}_i \cap \mathbb{U} \ne \Phi$ , the transition maps between the two are both $C^k$ -diffeomorphism.</p> <p>Two altases are $C^k$ compatible iff every chart of one is compatible with the other atlas. This is equivalent to saying that the union of the two $C^k$ atlases is still a $C^k$ atlas.</p> <p>An atlas $\mathcal{A}$ is maximal if it contains <em>all possible</em>  $C^k$ compatible atlases. The definition of a maximal atlas is needed so that two <strong>manifolds</strong> with different atlases, but which are $C^K$-compatible will not be considered different manifolds. A maximal $C^K$ atlas is what we call a $C^K$ differentiable structure. <a href="https://math.stackexchange.com/questions/1388864/i-am-confused-by-the-different-definitions-of-manifolds">source</a></p> <h3 id="ck-manifold-of-dimension-n">$C^k$ manifold of dimension n</h3> <p>A manifold $\mathcal{M}$ of dimension $n$ with a maximal $C^k$ atlas on it.</p> <h3 id="curve">Curve</h3> <p>In analytic geometry, a curve is continuous map from a one-dimensional space (interval) to an $n$-dimensional space: $\gamma: (-\epsilon, \epsilon) \rightarrow \mathcal{M}$. Namely, one can think of a curve in a topological space as being a “path” traced out continuously, where the trace is indexed by time. A differentiable curve is a differentiable manifold of dimension one.</p> <h3 id="tangent-space">Tangent Space</h3> <table> <tbody> <tr> <td>For a $d$-dimensional manifold $\mathcal{M}$, at every point $p$ on $\mathcal{M}$ there is a tagent space $\mathcal{T}_p \mathcal{M}$. Suppose we have two curves $\gamma_1: (-\epsilon_1, \epsilon_1) \rightarrow \mathcal{M}$ and $\gamma_2: (-\epsilon_2, \epsilon_2) \rightarrow \mathcal{M}$, and they both pass through the point $p$, i.e. $\gamma_1(0) = \gamma_2(0) = p$. If there is a chart $(\mathbb{U}, \varphi)$ containing $p$ while $\frac{d (\varphi \circ \gamma_1)}{dt}</td> <td>_{t=0} = \frac{d (\varphi \circ \gamma_2)}{dt}</td> <td>_{t=0}$ (derivatives at the mapped Euclidean space w.r.t curve trace time 0 are identical), then we call the two curves in an equivalence class, and we call the curves in the equivalence class <strong>tangent vectors</strong>, and all the tangent vectors form up a <strong>tangent space</strong>.</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="math"/><category term="math,"/><category term="flows,"/><category term="deep"/><category term="learning"/><summary type="html"><![CDATA[Basic Normalizing Flow]]></summary></entry><entry><title type="html">Generalized funtions</title><link href="https://stonezhng.github.io/blog/2021/generalized-function/" rel="alternate" type="text/html" title="Generalized funtions"/><published>2021-03-28T18:00:00+00:00</published><updated>2021-03-28T18:00:00+00:00</updated><id>https://stonezhng.github.io/blog/2021/generalized-function</id><content type="html" xml:base="https://stonezhng.github.io/blog/2021/generalized-function/"><![CDATA[<h1 id="functional"><strong>Functional</strong></h1> <p><a href="https://mathworld.wolfram.com/Functional.html">Functional</a> refers to a mapping from a space $\mathit{X}$ (usually of functions) into $\boldsymbol{R}$ (real numbers). (Or complex numbers, in a more general case.)</p> <h1 id="generalized-function"><strong>Generalized Function</strong></h1> <h2 id="definition">Definition</h2> <p>Let \(\varphi(x)\) be an <em>argument of the function</em>, which is a function where $x \in \boldsymbol{R}$, and $\forall x$, $\varphi(x) \in \mathscr{D}$.</p> <ul> <li> <p>$\varphi(x)$ is <em>smooth</em>, if it has derivatives of all orders.</p> </li> <li> <p>$\varphi(x)$ is <em>compact</em>, if it has a bounded <em>support</em>. The <em>support</em> of a function $\varphi(x)$ is written as $\text{supp}(\varphi)$, $\text{supp}(\varphi) := \{x \in \mathit{X} \mid \varphi(x) \ne 0 \}$.</p> </li> </ul> <p>Let $f(x) := \boldsymbol{R} \rightarrow \boldsymbol{R}$ be the kernel of $\varphi(x)$.</p> <p>We have the following <a href="https://mathworld.wolfram.com/LinearFunctional.html"><em>linear functional</em></a>:</p> \[T[\varphi] = \int f(x) \varphi(x) dx\] <ul> <li> <p>$T[\cdot]$ is <em>continuous</em>, if for any sequence $\{\varphi_k\}_{k = 1, \cdots, \infty}$, when the sequence converges to $\varphi$ at $k \rightarrow \infty$, the corresponding sequence $\{T[\varphi_k]\}$ converges to $T[\varphi]$.</p> </li> <li> <p>$T[\cdot]$ maps inputs from $\mathscr{D}$ (function space) to $\boldsymbol{R}$.</p> </li> </ul> <p><span style="color: green"> Any <em>linear functional</em> $T[\varphi]$, which is <em>continuous</em> on the set $\mathscr{D}$ of <em>smooth compact functions</em>, is called a <em>generalized function</em>. A <em>generalized function</em> is a <em>linear functional</em>, but not a <em>function</em>. </span></p> <h2 id="linearity-of-generalized-function">Linearity of Generalized Function</h2> <p>$\forall \varphi(x), \psi(x) \in \mathscr{D}$, $\forall a, b \in \boldsymbol{R}$, we have:</p> \[T[a\varphi + b\psi] = a T[\varphi] + b T[\psi]\] <h2 id="regular-generalized-function">Regular Generalized Function</h2> <p>If the kernal $f(x)$ is an everywhere continuous, bounded function, $T[\varphi]$ is called a <em>regular generalized function</em> identified with the kernal $f(x)$.</p> <h2 id="singular-generalized-function">Singular Generalized Function</h2> <p>A <em>singular generlized function</em> $T[\varphi]$ is still linear continuous, but its kernel is not continuous.</p> <p>The most important example is the function:</p> \[T[\varphi] = \varphi(0)\] <p>Though the kernel is not continuous, we still give it a symbol $\delta(x)$ and write it as:</p> \[T[\varphi] = \int \delta(x) \varphi(x) dx = \varphi(0)\] <p>Although the above thing is well known as the delta function, according to <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Wikipedia</a> and <a href="https://mathworld.wolfram.com/DeltaFunction.html">Wolfram</a>, given different contexts, the delta function can refer to different things. As a distribution, the delta function is $T[\cdot]$, the linear functional, and is written as $\delta[\varphi] = \varphi(0)$. As a measure, or in the engineering context, the kernel $\delta(x)$ in $\int \delta(x) \varphi(x) dx$ is the delta function. A very confusing expression is:</p> \[\delta [\varphi] = \int \delta(x) \varphi(x) dx = \varphi(0)\] <p>If $\varphi(x) = 1$:</p> \[\delta [1] = \int \delta(x) dx = 1\] <p>We can also “shift” the delta function to map the function space to function value at arbitrary points: (again the first $\delta_a$ is a functional delta, the second $\delta$ in the integral is a function delta)</p> \[\delta_a [\varphi] = \int \delta(x - a) \varphi(x) dx = \varphi(a)\] <p>One approach to estimate the delta function is by the limit of a sequence of ordinary integrals, where the $k$ th element in the sequence looks like:</p> \[\delta_k [\varphi] = \int \delta_k(x) \varphi(x) dx\] <p>$\delta_k(x)$ can be ordinary functions. One choice is the Gaussian probability density function: $\delta_k(x) = \frac{1}{k\sqrt{2 \pi}} e^{-\frac{x^2}{2k^2}}$</p> <h1 id="derivatives-of-generalized-function">Derivatives of Generalized Function</h1> <p>We define the derivative of a generalized function by the derivative of its kernel functions:</p> \[T'[\varphi] = \int f'(x) \varphi(x) dx\] <p>According to the rule of integration by parts, we have:</p> \[\int f'(x) \varphi(x) dx + \int f(x) \varphi'(x) dx = f(x)\varphi(x) |_{-\infty}^{\infty}\] <p>Recall that $\varphi(x)$ is compact (has a bounded support), so $\lim_{x \rightarrow \infty}\varphi(x) = 0$, $\lim_{x \rightarrow -\infty}\varphi(x) = 0$, therefore we have:</p> \[\int f'(x) \varphi(x) dx + \int f(x) \varphi'(x) dx = 0\] <p>Which means:</p> \[T'[\varphi] + T[\varphi'] = 0\] <p>So We can do derivative on $\varphi$ to get the derivative of a generalized function.</p> <p>With the same logic, we have</p> \[T^{(n)}[\varphi] = (-1)^nT[\varphi^{(n)}(x)]\] <p>The Leibniz formula also works on the product of a generalized function and a function. First we have the product of a function and a generalized function defined as:</p> \[g(x)T[\varphi] = T[g\varphi]\] <p>Then the Leibniz formula is:</p> \[(g(x)T[\varphi])' = g'(x)T[\varphi] + g(x)T'[\varphi]\] \[(g(x)T[\varphi])^{(n)} = \sum^{n}_{m = 0}g^{(m)}(x)T^{(n-m)}[\varphi]\] <p>This is more like a trick, since the symbol $g’(x)T[\varphi]$ is not an actual product, you just switch between different expressions to make it look like the Leibniz formula :</p> \[\begin{align*} (g(x)T[\varphi])' &amp;= T'[g\varphi] \\ &amp;= T[(g\varphi)'] \\ &amp;= T[g'\varphi + g\varphi'] \\ &amp;= T[g'\varphi] + T[g\varphi'] \text{ (linearity)} \\ &amp;= g'(x)T[\varphi] + g(x)T'[\varphi] \end{align*}\] <h1 id="generalized-function-of-a-composite-argument">Generalized Function of a Composite Argument</h1> <p>First we have the following equality:</p> \[\int f(g(x))\varphi(x) dx = \int f(y)\varphi(g^{-1}(y)) g^{-1 \prime} (y) dy\] <p>Where $g^{-1}(x)$ is the inverse of $g(x)$, i.e. $g(g^{-1}(x)) = x$. The above equality can be easily proved by replace $x$ on the left side with $g^{-1}(y)$.</p> <p>We name $\int f(g(x))\varphi(x) dx$ as a generalized function $T$ of a <em>composite argument</em> $g(x)$.</p>]]></content><author><name></name></author><category term="math"/><category term="math,"/><category term="algebra"/><summary type="html"><![CDATA[Functional]]></summary></entry><entry><title type="html">Pytorch Adam may update frozen parameters</title><link href="https://stonezhng.github.io/blog/2021/adam-may-update-frozen-param/" rel="alternate" type="text/html" title="Pytorch Adam may update frozen parameters"/><published>2021-03-25T20:12:00+00:00</published><updated>2021-03-25T20:12:00+00:00</updated><id>https://stonezhng.github.io/blog/2021/adam-may-update-frozen-param</id><content type="html" xml:base="https://stonezhng.github.io/blog/2021/adam-may-update-frozen-param/"><![CDATA[<p>I was working on a deep learning training task that needed to freeze part of the parameters after 10 epochs of training. With Adam optimizer, even if I set</p> <pre><code class="language-Python">for parameter in model:
    parameter.requires_grad = False
</code></pre> <p>There are still trivial differences before and after each epoch of training on those frozen parameters, like one can be from 0.1678 to 0.1674.</p> <p>According to <a href="https://discuss.pytorch.org/t/why-is-it-when-i-call-require-grad-false-on-all-my-params-my-weights-in-the-network-would-still-update/22126/15">this post</a>, Pytorch indeed has such an issue.</p> <p>A better practice is to split the training process into two parts like a pretraining-fine tuing work. In my situation,</p> <ul> <li>in the first part, train all the parameters and save them; (pretraining)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">...</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="p">...)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">checkpoint_path</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">need_freeze</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
        <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <ul> <li>in the second part, reinitiate the model, start with a new optimizer without the frozen parameters, load the trained parameters; (fine tuning)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>

<span class="sh">"""</span><span class="s">
load saved model dict
</span><span class="sh">"""</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">...</span>

<span class="n">filtered_params</span> <span class="o">=</span>  <span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">filtered_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="p">...)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">TOTAL_NUM</span> <span class="o">-</span> <span class="mi">10</span><span class="p">):</span>
    <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>to check if the parameters are indeed not updated:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="bp">...</span>
<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="bp">...</span>

<span class="n">filtered_params</span> <span class="o">=</span>  <span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">filtered_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="p">...)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">TOTAL_NUM</span> <span class="o">-</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">freeze_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="nf">need_freeze</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>

    <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

    <span class="n">updated_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="nf">need_freeze</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">freeze_params</span><span class="p">)):</span>
        <span class="n">eq_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">freeze_params</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">updated_params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">assert</span> <span class="n">eq_tensor</span><span class="p">.</span><span class="nf">all</span><span class="p">().</span><span class="n">data</span>


<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="coding"/><category term="deep"/><category term="learning,"/><category term="Pytorch"/><summary type="html"><![CDATA[I was working on a deep learning training task that needed to freeze part of the parameters after 10 epochs of training. With Adam optimizer, even if I set]]></summary></entry><entry><title type="html">Jekyll Installation on macOS Catalina</title><link href="https://stonezhng.github.io/blog/2021/jekyll-installation-on-macos/" rel="alternate" type="text/html" title="Jekyll Installation on macOS Catalina"/><published>2021-03-23T02:43:12+00:00</published><updated>2021-03-23T02:43:12+00:00</updated><id>https://stonezhng.github.io/blog/2021/jekyll-installation-on-macos</id><content type="html" xml:base="https://stonezhng.github.io/blog/2021/jekyll-installation-on-macos/"><![CDATA[<p>Due to new security features in macOS Mojave and later, the <a href="https://jekyllrb.com/docs/installation/macos/">tutorial on Jekyll website</a> will raise an error when running command: <code class="language-plaintext highlighter-rouge">gem install &lt;PACKAGE&gt;</code>:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">ERROR:  Loading command: install (LoadError)
    cannot load such file -- openssl
ERROR:  While executing gem ... (NoMethodError)
    undefined method `invoke_with_build_args' for nil:NilClass
</span></code></pre></div></div> <p>The solution is to reinstall ruby with openssl, as discussed in <a href="https://github.com/rvm/rvm/issues/4819">this thread</a>:</p> <ul> <li>Step 1: install openssl: <code class="language-plaintext highlighter-rouge">brew install rbenv/tap/openssl@1.0</code></li> <li>Step 2: install/reinstall ruby with openssl: <code class="language-plaintext highlighter-rouge">rvm install &lt;RUBY_VERSION&gt; --with-openssl-dir='/usr/local/opt/openssl@1.0'</code> (if reinstall, replace <code class="language-plaintext highlighter-rouge">install</code> with <code class="language-plaintext highlighter-rouge">reinstall</code>)</li> <li>Step 3: update gem: <code class="language-plaintext highlighter-rouge">gem update --system</code></li> </ul> <p>You are good to go! Have fun with gem!</p>]]></content><author><name></name></author><category term="coding"/><category term="jekyll"/><category term="macOS"/><summary type="html"><![CDATA[Due to new security features in macOS Mojave and later, the tutorial on Jekyll website will raise an error when running command: gem install &lt;PACKAGE&gt;:]]></summary></entry></feed>