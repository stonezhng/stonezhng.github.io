---
layout: post
title:  Introduction to Normalzing Flows
date:   2023-10-07 16:40:16
tags: math, flows, deep learning
categories: math
math: true
header-includes:
  - \usepackage{mathrsfs}
  - \usepackage{amsbsy}
---

# Basic Normalizing Flow

A **normalizing flow** is similar to a VAE in that we try to build up $p(X)$ by starting from a simple known distribution $p(Z)$. $X$ and $Z$ are of **same dimensionality**. 

The very basic idea of a flow connecting a complex $X$ and a simple $Z$ would be using a **bijective** function $f$ with its inverse $f^{-1}$ such that:
* $X = f(Z)$
* $Z = f^{-1}(X)$
*  $p(X) = p_Z(f^{-1}(X))|\text{delta}(\mathbf{J}_{f^{-1}})|$ 
where the term on the right is the absolute value of the determinant of the Jacobian of $f^{-1}(X)$ w.r.t. $X$, and $p_Z$ uses a subscription $Z$ to differentiate it from the distribution of $X$.

It is natural to introduce a chain structure between $X$ and $Z$:
* $X  = f_1(f_0(Z))$
* $Z = f_0^{-1}(f_1^{-1}(X))$
* $p(X) = p_Z(f_0^{-1}(f_1^{-1}(X))) \left\lvert \text{delta}(\mathbf{J}_{f_0^{-1}}) \right\lvert  \left\lvert \text{delta}(\mathbf{J}_{f_1^{-1}}) \left\right$

## Training Process

Since most of the time $X$ is what we observed and has an empirical distribution consisting of $N$ data points $\{X_1, X_2, \cdots, X_N\}$, we can simply optimize the negative loglikelihood:

$$\text{argmin}_f \sum_{i = 1}^N -\log (p(X_I)) =  \text{argmin}_f \sum_{i = 1}^N -\log (p_Z(f^{-1}(X_i))|\text{delta}(\mathbf{J}_{f^{-1}(X_i)})|)$$

## Commonly Used Flows

#### Planar Flow

$$X=f(Z)= Z+ \mathbf{u} h(\mathbf{w}^TZ+b)$$

$\mathbf{u}, \mathbf{w}$ and $b$ are parameters. Additional constraints are required on $\mathbf{u}, \mathbf{w}, b$ and $h$ to guarantee the function is bijective. For example, it is ituitive that $h$ needs to be bijective,  like $h = \tanh$.

The Jabobian of $f^{-1}(X)$  is not obvious, as it depends on $h$, so the analytic form of $p(X)$ is not easy to compute.

### Nonlinear Independent Components Estimation (NICE)

This method requires that $X$ can be split into two disjoint parts, $X_1$ and $X_2$, i.e. $X = [X_1, X_2]$. Same assumption for $Z$.

Forward mapping $f: Z \rightarrow X$
* $X_1 = Z_1$
* $X_2 = Z_2 + m_\theta(Z_1)$, where $m_\theta$ is a neural network
Inverse mapping $f^{-1}: X \rightarrow Z$ 
* $Z_1 = X_1$
* $Z_2 = X_2 - m_\theta(X_1)$
The  inverse mapping  of $Z_2$ can be simply obtained by replacing $Z_1$ in forward mapping of $X_2$ with $X_1$ and move $m_\theta(Z_1)$  (equivalent to $m_\theta(X_1)$) to LHS. 

The Jacobian of the forward mapping is lower trangular, whose determinant is simply the product of the elements on the diagonal, i.e. 

$$\frac{\partial f^{-1}(X)}{\partial X} = \begin{pmatrix}  
\frac{\partial (Z_1)}{\partial X_1} & \frac{\partial (Z_1)}{\partial X_2} \\  
\frac{\partial (Z_2)}{\partial X_1} & \frac{\partial (Z_2)}{\partial X_2}  
\end{pmatrix} =\begin{pmatrix}  
I & 0 \\  
\frac{\partial (-m_\theta(X_1))}{\partial X_1} & I   
\end{pmatrix}$$

$$\text{delta}(\begin{pmatrix}  
I & 0 \\  
\frac{\partial (-m_\theta(X_1))}{\partial X_1} & I   
\end{pmatrix}) = 1$$

Therefore, this defines a volume preserving transformation.

# Continuous Normalizing Flows

[Original paper](https://browse.arxiv.org/pdf/1806.07366.pdf)

Continuous Normalizing Flows solves the problem of selecting proper transition functions and the high computation complexity of Jacobians. The idea first origniates from a discrete sequence based transition:

$$X_{t+1} = X_t + f(X_t, \theta_t)$$

where $f$ is time-sensitive with the parameter $\theta_t$ changing with time. From a perspective of flows, this sequence transition provides a natural way of connecting $X$ and $Z$, i.e. $X$ being $X_T$ at the end of the sequence and $Z$ being $X_0$ at the beginning of the sequence, with finite discrete intermediate states $X_t, t \in [1, T-1] \cap \mathbb{N}^+$. The transition is obviously bijective, since each step is a simple linear function.

We then want to see what if the sequence becomes continous. Since the discrete sequence transition is describing the change at time $t$ in a discrete manor, i.e. $X_{t+1} - X_t = \Delta X_{t} =f(X_t, \theta_t)$, the continous version is then: $$\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)$$
which is an ordinary differential equation (ODE). Starting from the input layer$X_0$ ($Z$), we can define the output layer$X_T$ to be the solution to this ODE initial value problem at some time $T$ and emprically computed by some black box ODE solver: 

$$X_t = X_0 + \int_0^tf(X_i, i, \theta)di = \text{ODESolve}(X_0, f, 0, t, θ)$$

## Computation of Jacobian

With the ODE to define the continous mapping from $X_t$ to $X_{t+1}$, the next question is how the probablity would change from $t$ to $t+1$. In the discrete case, we have

$$\log (p(X_{t+1})) - \log(p(X_t)) = \log(|\text{delta}(\mathbf{J}_{F_t^{-1}})|)$$ , 

where $F_t$ is the mapping function at time $t$: $X_{t+1} = X_{t} + \int_t^{t+1}f(X_i, i, \theta)di = \text{ODESolve}(X_t, f, t, t+1, θ)$.

The paper proved that in the continous case, 

$$\frac{\partial \log(p(X_t))}{\partial t} = -\text{tr}(\frac{\partial f}{\partial X_t})$$

with same $f$ defined in the ODE $\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)$. ($f$ is of the same dimensionality as $X_t$ so its first derivative w.r.t $X_t$ is a square matrix, and this theorem says we can only compute the diagnal elements and sum them up)

An example made by the author is a continous version of Planar Flow, where the function $\mathbf{u}h(\cdot)$ is not served as a direct mapping from $X_t$ to $X_{t+1}$, but describing the gradient (dynamic) at $X_t$:

$$\frac{\partial X_t}{\partial t} = \mathbf{u}h(\mathbf{w}^TX_t + b)$$

$$\frac{\partial \log(p(X_t))}{\partial t} = -\mathbf{u}^T\frac{\partial h}{\partial X_t}$$

The two ODEs then give way of sampling complex $X$ ($X_t$)from simple random variable $Z$ ($X_0$) by the first ODE and estimate its density by the second ODE, assuming we have a good black box ODE solver.  

The author did two addtional things to the two ODEs. 
* Because the trace function is linear, we can easily add multiple $f$s to ODE:

$$\frac{\partial X_t}{\partial t} = \sum_{i=1}^N f_i(X_t, t, \theta)$$

$$\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\frac{\partial f_i}{\partial X_t})$$

* Also by utilizing the linearity, we can specify the role of $t$ in $f$ in a gating mechanism manner:

$$\frac{\partial X_t}{\partial t} = \sum_{i=1}^N \sigma_i(t)f_i(X_t, \theta)$$

$$\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\sigma_i(t)\frac{\partial f_i}{\partial X_t})$$

where $f_i$ now is independent of $t$

## Backpropogation

A vanilla way of computing the gradients of all $f$ is expensive. The author proposed  using the adjoint sensitivity method, which computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers.

Suppose we have a scalar loss function $L$ on the output $X_{t_1} = \text{ODESolve}(X_{t_0}, f, t_0, t_1, θ)$, written as $$L(\text{ODESolve}(X_{t_0}, f, t_0, t_1, θ))$$The target is $\frac{\partial L}{\partial \theta}$.

We first work on the adjoint $$\mathbf{a}(t) = \frac{\partial L}{\partial X_{t}}$$Its gradients (dynamics) are given by another ODE, which can be thought of as the instantaneous analog of the chain rule: 

$$\frac{\partial \mathbf{a}(t)}{\partial t} = -\mathbf{a(t)}^T\frac{\partial f(X_t, t, \theta)}{\partial X_{t}}$$

We can then compute $\mathbf{a}(t)$ by another call to an ODE solver.  This solver **must run backwards,** starting from the initial value of $\mathbf{a}(t_1)$. Because we need to know $X_t$ when computing this gradient at time $t$, we need to reversely get $X_t$ starting from $t_1$ together with the backward computation of $\mathbf{a}(t)$. 

Finally we use $\mathbf{a}(t)$ to compute $\frac{\partial L}{\partial \theta}$:

$$\frac{\partial L}{\partial \theta} = -\int_{t_0}^{t_1}\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta} dt$$

which is another ODE:

$$\frac{\partial \frac{\partial L}{\partial \theta}}{\partial t} = -\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta}$$

so call ODESolver again. 

# Riemannian Continuous Normalizing Flows

[Original paper](https://proceedings.neurips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf)

In this work, flows are defined via vector fields on manifolds and computed as the solution to the associated ordinary differential equation (ODE). Intuitively, this method operates by first parametrizing a vector field on the manifold with a neural network, then sampling particles from a base distribution, and finally approximating their flow along the vector field using a numerical solver. The high level idea is, for the ODE of the dynamic:
$$\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)$$
We are now considering $f(X_t, t, \theta)$ as a vector field, and it is describing the velocity at $X_t$. 