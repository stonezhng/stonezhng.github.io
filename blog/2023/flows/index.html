<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Introduction to Normalzing Flows | Sidong Zhang</title> <meta name="author" content="Sidong Zhang"> <meta name="description" content="Hi this is Sidong (or you can just call me Stone). I am recording my research projects, ideas and learning notes in this website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/infofusion.png?5c67728fce50989ea09210456b6a4e07"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stonezhng.github.io/blog/2023/flows/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sidong </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to Normalzing Flows</h1> <p class="post-meta">October 7, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/math"> <i class="fas fa-hashtag fa-sm"></i> math,</a>   <a href="/blog/tag/flows"> <i class="fas fa-hashtag fa-sm"></i> flows,</a>   <a href="/blog/tag/deep"> <i class="fas fa-hashtag fa-sm"></i> deep</a>   <a href="/blog/tag/learning"> <i class="fas fa-hashtag fa-sm"></i> learning</a>     ·   <a href="/blog/category/math"> <i class="fas fa-tag fa-sm"></i> math</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="basic-normalizing-flow">Basic Normalizing Flow</h1> <p>A <strong>normalizing flow</strong> is similar to a VAE in that we try to build up $p(X)$ by starting from a simple known distribution $p(Z)$. $X$ and $Z$ are of <strong>same dimensionality</strong>.</p> <p>The very basic idea of a flow connecting a complex $X$ and a simple $Z$ would be using a <strong>bijective</strong> function $f$ with its inverse $f^{-1}$ such that:</p> <ul> <li>$X = f(Z)$</li> <li>$Z = f^{-1}(X)$</li> <li>$p(X) = p_Z(f^{-1}(X))|\text{delta}(\mathbf{J}_{f^{-1}})|$ where the term on the right is the absolute value of the determinant of the Jacobian of $f^{-1}(X)$ w.r.t. $X$, and $p_Z$ uses a subscription $Z$ to differentiate it from the distribution of $X$.</li> </ul> <p>It is natural to introduce a chain structure between $X$ and $Z$:</p> <ul> <li>$X = f_1(f_0(Z))$</li> <li>$Z = f_0^{-1}(f_1^{-1}(X))$</li> <li>$p(X) = p_Z(f_0^{-1}(f_1^{-1}(X))) \left\lvert \text{delta}(\mathbf{J}<em>{f_0^{-1}}) \right\lvert \left\lvert \text{delta}(\mathbf{J}</em>{f_1^{-1}}) \left\right$</li> </ul> <h2 id="training-process">Training Process</h2> <p>Since most of the time $X$ is what we observed and has an empirical distribution consisting of $N$ data points ${X_1, X_2, \cdots, X_N}$, we can simply optimize the negative loglikelihood:</p> \[\text{argmin}_f \sum_{i = 1}^N -\log (p(X_I)) = \text{argmin}_f \sum_{i = 1}^N -\log (p_Z(f^{-1}(X_i))|\text{delta}(\mathbf{J}_{f^{-1}(X_i)})|)\] <h2 id="commonly-used-flows">Commonly Used Flows</h2> <h4 id="planar-flow">Planar Flow</h4> \[X=f(Z)= Z+ \mathbf{u} h(\mathbf{w}^TZ+b)\] <p>$\mathbf{u}, \mathbf{w}$ and $b$ are parameters. Additional constraints are required on $\mathbf{u}, \mathbf{w}, b$ and $h$ to guarantee the function is bijective. For example, it is ituitive that $h$ needs to be bijective, like $h = \tanh$.</p> <p>The Jabobian of $f^{-1}(X)$ is not obvious, as it depends on $h$, so the analytic form of $p(X)$ is not easy to compute.</p> <h3 id="nonlinear-independent-components-estimation-nice">Nonlinear Independent Components Estimation (NICE)</h3> <p>This method requires that $X$ can be split into two disjoint parts, $X_1$ and $X_2$, i.e. $X = [X_1, X_2]$. Same assumption for $Z$.</p> <p>Forward mapping $f: Z \rightarrow X$</p> <ul> <li>$X_1 = Z_1$</li> <li>$X_2 = Z_2 + m_\theta(Z_1)$, where $m_\theta$ is a neural network Inverse mapping $f^{-1}: X \rightarrow Z$</li> <li>$Z_1 = X_1$</li> <li>$Z_2 = X_2 - m_\theta(X_1)$ The inverse mapping of $Z_2$ can be simply obtained by replacing $Z_1$ in forward mapping of $X_2$ with $X_1$ and move $m_\theta(Z_1)$ (equivalent to $m_\theta(X_1)$) to LHS.</li> </ul> <p>The Jacobian of the forward mapping is lower trangular, whose determinant is simply the product of the elements on the diagonal, i.e.</p> \[\frac{\partial f^{-1}(X)}{\partial X} = \begin{pmatrix} \frac{\partial (Z_1)}{\partial X_1} &amp; \frac{\partial (Z_1)}{\partial X_2} \\ \frac{\partial (Z_2)}{\partial X_1} &amp; \frac{\partial (Z_2)}{\partial X_2} \end{pmatrix} =\begin{pmatrix} I &amp; 0 \\ \frac{\partial (-m_\theta(X_1))}{\partial X_1} &amp; I \end{pmatrix}\] \[\text{delta}(\begin{pmatrix} I &amp; 0 \\ \frac{\partial (-m_\theta(X_1))}{\partial X_1} &amp; I \end{pmatrix}) = 1\] <p>Therefore, this defines a volume preserving transformation.</p> <h1 id="continuous-normalizing-flows">Continuous Normalizing Flows</h1> <p><a href="https://browse.arxiv.org/pdf/1806.07366.pdf" rel="external nofollow noopener" target="_blank">Original paper</a></p> <p>Continuous Normalizing Flows solves the problem of selecting proper transition functions and the high computation complexity of Jacobians. The idea first origniates from a discrete sequence based transition:</p> \[X_{t+1} = X_t + f(X_t, \theta_t)\] <p>where $f$ is time-sensitive with the parameter $\theta_t$ changing with time. From a perspective of flows, this sequence transition provides a natural way of connecting $X$ and $Z$, i.e. $X$ being $X_T$ at the end of the sequence and $Z$ being $X_0$ at the beginning of the sequence, with finite discrete intermediate states $X_t, t \in [1, T-1] \cap \mathbb{N}^+$. The transition is obviously bijective, since each step is a simple linear function.</p> <p>We then want to see what if the sequence becomes continous. Since the discrete sequence transition is describing the change at time $t$ in a discrete manor, i.e. $X_{t+1} - X_t = \Delta X_{t} =f(X_t, \theta_t)$, the continous version is then: \(\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)\) which is an ordinary differential equation (ODE). Starting from the input layer$X_0$ ($Z$), we can define the output layer$X_T$ to be the solution to this ODE initial value problem at some time $T$ and emprically computed by some black box ODE solver:</p> \[X_t = X_0 + \int_0^tf(X_i, i, \theta)di = \text{ODESolve}(X_0, f, 0, t, θ)\] <h2 id="computation-of-jacobian">Computation of Jacobian</h2> <p>With the ODE to define the continous mapping from $X_t$ to $X_{t+1}$, the next question is how the probablity would change from $t$ to $t+1$. In the discrete case, we have</p> <table> <tbody> <tr> <td>$$\log (p(X_{t+1})) - \log(p(X_t)) = \log(</td> <td>\text{delta}(\mathbf{J}_{F_t^{-1}})</td> <td>)$$ ,</td> </tr> </tbody> </table> <p>where $F_t$ is the mapping function at time $t$: $X_{t+1} = X_{t} + \int_t^{t+1}f(X_i, i, \theta)di = \text{ODESolve}(X_t, f, t, t+1, θ)$.</p> <p>The paper proved that in the continous case,</p> \[\frac{\partial \log(p(X_t))}{\partial t} = -\text{tr}(\frac{\partial f}{\partial X_t})\] <p>with same $f$ defined in the ODE $\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)$. ($f$ is of the same dimensionality as $X_t$ so its first derivative w.r.t $X_t$ is a square matrix, and this theorem says we can only compute the diagnal elements and sum them up)</p> <p>An example made by the author is a continous version of Planar Flow, where the function $\mathbf{u}h(\cdot)$ is not served as a direct mapping from $X_t$ to $X_{t+1}$, but describing the gradient (dynamic) at $X_t$:</p> \[\frac{\partial X_t}{\partial t} = \mathbf{u}h(\mathbf{w}^TX_t + b)\] \[\frac{\partial \log(p(X_t))}{\partial t} = -\mathbf{u}^T\frac{\partial h}{\partial X_t}\] <p>The two ODEs then give way of sampling complex $X$ ($X_t$)from simple random variable $Z$ ($X_0$) by the first ODE and estimate its density by the second ODE, assuming we have a good black box ODE solver.</p> <p>The author did two addtional things to the two ODEs.</p> <ul> <li>Because the trace function is linear, we can easily add multiple $f$s to ODE:</li> </ul> \[\frac{\partial X_t}{\partial t} = \sum_{i=1}^N f_i(X_t, t, \theta)\] \[\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\frac{\partial f_i}{\partial X_t})\] <ul> <li>Also by utilizing the linearity, we can specify the role of $t$ in $f$ in a gating mechanism manner:</li> </ul> \[\frac{\partial X_t}{\partial t} = \sum_{i=1}^N \sigma_i(t)f_i(X_t, \theta)\] \[\frac{\partial \log(p(X_t))}{\partial t} = \sum_{i=1}^N-\text{tr}(\sigma_i(t)\frac{\partial f_i}{\partial X_t})\] <p>where $f_i$ now is independent of $t$</p> <h2 id="backpropogation">Backpropogation</h2> <p>A vanilla way of computing the gradients of all $f$ is expensive. The author proposed using the adjoint sensitivity method, which computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers.</p> <p>Suppose we have a scalar loss function $L$ on the output $X_{t_1} = \text{ODESolve}(X_{t_0}, f, t_0, t_1, θ)$, written as \(L(\text{ODESolve}(X_{t_0}, f, t_0, t_1, θ))\)The target is $\frac{\partial L}{\partial \theta}$.</p> <p>We first work on the adjoint \(\mathbf{a}(t) = \frac{\partial L}{\partial X_{t}}\)Its gradients (dynamics) are given by another ODE, which can be thought of as the instantaneous analog of the chain rule:</p> \[\frac{\partial \mathbf{a}(t)}{\partial t} = -\mathbf{a(t)}^T\frac{\partial f(X_t, t, \theta)}{\partial X_{t}}\] <p>We can then compute $\mathbf{a}(t)$ by another call to an ODE solver. This solver <strong>must run backwards,</strong> starting from the initial value of $\mathbf{a}(t_1)$. Because we need to know $X_t$ when computing this gradient at time $t$, we need to reversely get $X_t$ starting from $t_1$ together with the backward computation of $\mathbf{a}(t)$.</p> <p>Finally we use $\mathbf{a}(t)$ to compute $\frac{\partial L}{\partial \theta}$:</p> \[\frac{\partial L}{\partial \theta} = -\int_{t_0}^{t_1}\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta} dt\] <p>which is another ODE:</p> \[\frac{\partial \frac{\partial L}{\partial \theta}}{\partial t} = -\mathbf{a}(t)^T\frac{\partial f(X_t, t, \theta)}{\partial \theta}\] <p>so call ODESolver again.</p> <h1 id="riemannian-continuous-normalizing-flows">Riemannian Continuous Normalizing Flows</h1> <p><a href="https://proceedings.neurips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf" rel="external nofollow noopener" target="_blank">Original paper</a></p> <p>In this work, flows are defined via vector fields on manifolds and computed as the solution to the associated ordinary differential equation (ODE). Intuitively, this method operates by first parametrizing a vector field on the manifold with a neural network, then sampling particles from a base distribution, and finally approximating their flow along the vector field using a numerical solver. The high level idea is, for the ODE of the dynamic: \(\frac{\partial X_t}{\partial t} = f(X_t, t, \theta)\) We are now considering $f(X_t, t, \theta)$ as a vector field, and it is describing the velocity at $X_t$.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/generalized-function/">Generalized funtions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/adam-may-update-frozen-param/">Pytorch Adam may update frozen parameters</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/jekyll-installation-on-macos/">Jekyll Installation on macOS Catalina</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Sidong Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>