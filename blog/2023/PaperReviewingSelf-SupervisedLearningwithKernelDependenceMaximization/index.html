<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Paper Reviewing - Self-Supervised Learning with Kernel Dependence Maximization | Sidong Zhang</title> <meta name="author" content="Sidong Zhang"> <meta name="description" content="Hi this is Sidong (or you can just call me Stone). I am recording my research projects, ideas and learning notes in this website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/infofusion.png?5c67728fce50989ea09210456b6a4e07"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stonezhng.github.io/blog/2023/PaperReviewingSelf-SupervisedLearningwithKernelDependenceMaximization/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sidong </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Reviewing - Self-Supervised Learning with Kernel Dependence Maximization</h1> <p class="post-meta">November 24, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/math"> <i class="fas fa-hashtag fa-sm"></i> math,</a>   <a href="/blog/tag/metric"> <i class="fas fa-hashtag fa-sm"></i> metric,</a>   <a href="/blog/tag/deep"> <i class="fas fa-hashtag fa-sm"></i> deep</a>   <a href="/blog/tag/learning"> <i class="fas fa-hashtag fa-sm"></i> learning</a>     ·   <a href="/blog/category/math"> <i class="fas fa-tag fa-sm"></i> math</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>https://proceedings.neurips.cc/paper_files/paper/2021/file/83004190b1793d7aa15f8d0d49a13eba-Paper.pdf</p> <h1 id="metric-space">Metric Space</h1> <p>A metric space is a set with a notion of distance defined between every pair of elements in the set, the distance is called metric or distance function. A complete metric space is a metric space where a Cauchy sequence of any point in the metric space is also in the same metric space. Examples: on the real number set interval (0, 1) with absolute difference metric, a sequence $x_n = \frac{1}{n}$, the sequence is Cauchy, but it converges to 0 which is outside of the interval, so the metric space is not complete.</p> <h2 id="hilbert-space">Hilbert Space</h2> <p>A Hilbert space is a <a href="https://en.wikipedia.org/wiki/Vector_space" title="Vector space" rel="external nofollow noopener" target="_blank">vector space</a> equipped with an <a href="https://en.wikipedia.org/wiki/Inner_product" title="Inner product" rel="external nofollow noopener" target="_blank">inner product</a> that induces a <a href="https://en.wikipedia.org/wiki/Distance_function" rel="external nofollow noopener" target="_blank">distance function</a> for which the space is a <a href="https://en.wikipedia.org/wiki/Complete_metric_space" title="Complete metric space" rel="external nofollow noopener" target="_blank">complete metric space</a>.</p> <table> <tbody> <tr> <td>Hilbert space adds an additional constraint on how the distance function is defined on a complete metric space. For example, on $\mathbb{R}^2$ with inner product $(x_1, x_2) \cdot (y_1, y_2) = x_1y_1 + x_2y_2$ , a norm $</td> <td> </td> <td>x</td> <td> </td> <td>$ is defined by $</td> <td> </td> <td>x</td> <td> </td> <td>= \sqrt{x \cdot x}$, and the distance between two points is defined b $</td> <td> </td> <td>x - y</td> <td> </td> <td>$, which is just the common Euclidean distance on $\mathbb{R}^2$, but constructed from perspective of inner product.</td> </tr> </tbody> </table> <h2 id="reproducing-hilbert-kernel-space">Reproducing Hilbert Kernel Space</h2> <p>https://www.youtube.com/watch?v=EoM_DF3VAO8 https://www.youtube.com/watch?v=ABOEE3ThPGQ</p> <h3 id="kernel">Kernel</h3> <p>The idea of kernel is to connecting between linear space and high dimensional space. Suppose we have a random space $\mathcal{X}$ , and we have a function $\phi: \mathcal{X} \rightarrow \mathcal{H}$ that maps a point in $\mathcal{X}$ to a point in another space $\mathcal{H}$. The new space $\mathcal{H}$ is a Hilbert space which guarantees some good properties, and a typical example of a Hilbert space is $\mathbb{R}^d$ (d-dimensional Euclidean space). In machine learning, the intuition of seeking such a mapping function $\phi$ is that the classification algorithms such as SVM are easier to optimized on $\mathbb{R}^d$, which means with $\phi$ we can run the algorithms as efficiently as on $\mathbb{R}^d$ with data samples collected from more complex spaces.</p> <p>Because $\phi(x) \in \mathcal{H}$, it is natural that we look at the inner product between $\phi(x)$ and $\phi(y)$, i.e. $\phi(x) \cdot \phi(y)$. We hope there is a direct way to get this inner product scalar value from $\mathcal{X}$ instead of computing $\phi$, so we propose a new function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, such that $k(x, y) = \phi(x) \cdot \phi(y)$. This approach to compute inner product directly is called kernel trick, and $k$ is called a kernel function.</p> <p>A kernel function always satisfies the following properties:</p> <ul> <li>Symmetry. $k(x, y) = k(y, x)$. Since the inner product on $\mathcal{H}$ is symmetric.</li> <li>Has a corresponding positive semi-definite kernel metric. We construct a kernel metric $K$ on any $n$ data points $x_1, x_2, \cdots, x_n \in \mathcal{X}$ by setting $K_{i, j} = k(x_i, x_j)$. $K$ is positive semi-definite, which means that $\forall c \in \mathbb{R}^n$, $c^TKc \ge 0$</li> </ul> <h3 id="constructing-reproducing-kernel-hilbert-space">Constructing Reproducing Kernel Hilbert Space</h3> <p>Reproducing Kernel Hilbert Space answers the following question : given a set $\mathcal{X}$ and a kernel function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, how do we construct a Hilbert space $\mathcal{H}$ and the mapping function $\phi$ ? Basically this is an inverse version of how a kernel function is constructed, i.e. how we find the two component (a Hilbert space and a mapping function) that can give us the known $k$ .</p> <p>We start by define the following function</p> \[k_x(\cdot) = k(x, \cdot)\] <table> <tbody> <tr> <td>Using $k_x(\cdot)$, we map $x \in \mathcal{X}$ to an image of a function $k_x$. We then construct a spamming set $\mathcal{G}$ from ${k_x</td> <td>x \in \mathcal{X} }$ by taking all $k_x$ as elements and compute all of the finite linear combination of them:</td> </tr> </tbody> </table> \[\mathcal{G} = \{\sum_{i=1}^r m_i k_{x_i} | m_i \in \mathbb{R}, r \in \mathbb{N} \}\] <p>$\mathcal{G}$ is quite close to the Hilbert space we are looking for, but we need to do two things:</p> <ul> <li>Define the inner product. The way we define the inner product is</li> </ul> <p>\(k_{x_i} \cdot k_{x_j} = k(x_i, x_j)\) for $x_i \in \mathcal{X}$ and $x_i \in \mathcal{X}$</p> \[\sum_{i=1}^r \alpha_i k_{x_i} \cdot \sum_{j=1}^l \beta_j k_{y_j} = \sum_{i, j} \alpha_i\beta_jk(x_i, y_j)\] <p>Assuming that all $x_i \in \mathcal{X}$ and all $y_j \in \mathcal{X}$</p> <ul> <li>Add all the complement to make it a complete space, i.e. add all limits of Cauchy sequences.</li> </ul> <p>We now call this new $\bar{\mathcal{G}}$ a Reproducing Kernel Hilbert Space $\mathcal{H}$, and by constructing it satisfies $k(x, y) = \phi(x) \cdot \phi(y)$</p> <p>One key property of RHKS is the <strong>reproducing property</strong>:</p> <p>Let $f = \sum_{i=1}^r m_i k_{x_i}$, i.e. an element in RHKS, then $f \cdot k_x = f(x)$ . Notice the difference between $f$ and $f(x)$ here, the LHS is the inner product on two elements in RHKS, i.e. two images, while the RHS is a function value “reproducing” one of the function on LHS.</p> <h1 id="issues-of-infonce-in-self-supervised-learning">Issues of InfoNCE in self supervised learning</h1> <p>https://lilianweng.github.io/posts/2021-05-31-contrastive/</p> <p>InfoNCE performance cannot be explained solely by the properties of the mutual information, but is influenced more by other factors, such as the formulation of the estimator and the architecture of the feature extractor. Essentially, representations with the same MI can have drastically different representational qualities.</p> <p>The example in the paper:</p> <table> <tbody> <tr> <td>Suppose we have two inputs $a$ and $b$, and an encoder $E(\cdot</td> <td>M)$ parameterized by an integer $M$ that maps $a$ uniformly randomly to ${0, 2, \cdots, 2M}$ and $b$ uniformly randomly to ${1, 3, \cdots, 2M+1}$. Let us denote $z_a = E(a</td> <td>M)$ and $z_b = E(b</td> <td>M)$,</td> </tr> </tbody> </table> \[\text{MI}(z_a; z_b) = \sum_{z_a, z_b}p(z_a, z_b)\log \frac{p(z_a, z_b)}{p(z_a)p(z_b)}\] <p>For the joint distribution, $p(z_a=m_a, z_b=m_b) = \frac{1}{(M+1)^2}$</p> <h1 id="hilbert-schmidt-independence-criterion-hsic">Hilbert-Schmidt Independence Criterion (HSIC)</h1> <p>Suppose we have two input space $\mathcal{X}$ and $\mathcal{Y}$ , and we have two mapping functions $\phi: \mathcal{X} \rightarrow \mathcal{F}$ and $\psi: \mathcal{Y} \rightarrow \mathcal{G}$ , $\mathcal{F}$ and $\mathcal{G}$ being two reproducing kernel Hilbert spaces. HSIC is defined as:</p> \[\text{HSIC}(X; Y) = ||\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]||^2_{\text{HS}}\] <table> <tbody> <tr> <td>Which is the norm of the covariance between the mapped elements. The Hilbert-Schmidt norm $</td> <td> </td> <td>\cdot</td> <td> </td> <td>_{\text{HS}}$ in finite dimensions is the usual Frobenius norm ( the square root of the sum of the squares of its elements).</td> </tr> </tbody> </table> <p>Remember that the mapping function $\phi(X)$ and the corresponding inner product on RKHS can be represented by the kernel function, so by expanding the expression of HSIC we get:</p> \[\begin{align} \text{HSIC}(X; Y) &amp;= ||\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]||^2_{\text{HS}} \\ &amp;= &lt;(\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]), (\mathbb{E}[\phi(X) \cdot \psi(Y)] - \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)])&gt;_{\text{HS}} \\ &amp;= &lt;\mathbb{E}[\phi(X) \cdot \psi(Y)], \mathbb{E}[\phi(X) \cdot \psi(Y)]&gt;_{\text{HS}} \\ &amp; - 2&lt;\mathbb{E}[\phi(X) \cdot \psi(Y)], \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]&gt;_\text{HS} \\ &amp; + &lt;\mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)], \mathbb{E}[\phi(X)] \cdot \mathbb{E}[\psi(Y)]&gt;_\text{HS} \\ &amp;= \mathbb{E}&lt;[\phi(X) \cdot \psi(Y)], [\phi(X') \cdot \psi(Y')]&gt;_{\text{HS}} \\ &amp; - 2\mathbb{E}&lt;[\phi(X) \cdot \psi(Y)], [\phi(X')] \cdot \mathbb{E}[\psi(Y'')]&gt;_\text{HS} \\ &amp; + &lt;\mathbb{E}[\phi(X) \cdot \phi(X')], \mathbb{E}[\psi(Y) \cdot \psi(Y')]&gt;_\text{HS} \\ &amp;= \mathbb{E}[k(X, X')l(Y, Y')] - 2\mathbb{E}[k(X, X')l(Y, Y'')] + \mathbb{E}[k(X, X')]\mathbb{E}[l(Y, Y')]] \end{align}\] <p>assuming that $k$ and $l$ are two kernel functions on the two RKHS.</p> <p>Given $N$ iid drawn samples ${(X_1, Y_1), (X_2, Y_2), \cdots, (X_N, Y_N)}$, an empirical estimator is proposed by Gretton et al.</p> \[\widehat{\text{HSIC}}(X; Y) = \frac{1}{(N-1)^2}\text{Tr}(KHLH)\] <p>where $K$ and $L$ are the kernel matrices of $k$ and $l$ respectively on the $N$ data points from $\mathcal{X}$ and $\mathcal{Y}$, and $H = I - \frac{1}{N}11^T$ is called the centering matrix.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/flows/">Introduction to Normalizing Flows</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/SequentialLMM/">Reviewing the paper Longitudinal data analysis using matrix completion</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/generalized-function/">Generalized funtions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Uploading-Custom-Dataset-to-HuggingFace-with-Croissant-Metafile/">Uploading Custom Dataset to HuggingFace with Croissant Metafile</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/adam-may-update-frozen-param/">Pytorch Adam may update frozen parameters</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sidong Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>