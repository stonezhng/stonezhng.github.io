<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reviewing the paper Longitudinal data analysis using matrix completion | Sidong Zhang</title> <meta name="author" content="Sidong Zhang"> <meta name="description" content="Hi this is Sidong (or you can just call me Stone). I am recording my research projects, ideas and learning notes in this website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/infofusion.png?5c67728fce50989ea09210456b6a4e07"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stonezhng.github.io/blog/2023/SequentialLMM/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sidong </span>Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reviewing the paper Longitudinal data analysis using matrix completion</h1> <p class="post-meta">October 16, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/math"> <i class="fas fa-hashtag fa-sm"></i> math,</a>   <a href="/blog/tag/machine"> <i class="fas fa-hashtag fa-sm"></i> machine</a>   <a href="/blog/tag/learning"> <i class="fas fa-hashtag fa-sm"></i> learning</a>     ·   <a href="/blog/category/math"> <i class="fas fa-tag fa-sm"></i> math</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Let $\mathbf{b}(t) = (b_1(t), b_2(t), \cdots, b_K(t))$ be a vector of $K$ basis elements at timepoint $t$. It is considered a truncated basis because the assumption of $K$ elements may not sufficiently cover the full vector space.</p> <h1 id="sequential-lmm">Sequential LMM</h1> <p>The general goal is to get a mapping from time $\mathbf{t}_i$ to target space $\mathbf{y}_i$ for every observed sample $i \in {1, 2, \cdots, N}$.</p> <p>The paper called a mean at time t, i.e. $\mu(t)$ a <em>fixed effect</em>. $\mu(t)$ is defined as a weighted summary on the truncated vector space $\mathbf{b}(t)$ of length $K$, i.e.</p> \[\mu(t) = \mathbf{m} \cdot \mathbf{b}(t)\] <p>where $\mathbf{m} = (m_1, m_2, \cdots, m_K)$, $\mathbf{b}(t) = (b_1(t), b_2(t), \cdots, b_K(t))$</p> <p>The paper then introduces a per-sample coefficient , aka the individual <em>random effect</em>, $\mathbf{w}$. $\mathbf{w}$ can be an $N \times K$ matrix, where each row is for a data sample. For a sample’s coefficient $\mathbf{w}<em>i, i\in {1, 2, \cdots, N}$ of dim $K$, we assume that there are $n_i$ time steps for this sample, i.e. the time sequence looks like ${t</em>{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ of length $n_i$, and at every time step $t_{i, j}$ there is a fixed effect $\mu(t_{i, j})$ , leading to a $n_i$ - dim vector of $(\mu(t_{i, 1}), \mu(t_{i, 2}) \cdots, \mu(t_{i, n_i}))$ as the fixed effect for over all this individual’s time sequence. We denote this fixed effect vector of dim $n_i$ over this $i-$th individual sample as $\boldsymbol{\mathbf{\mu}}<em>i$. For each scalar $\mu(t</em>{i, j})$, it follows the same definition above and takes the truncated basic vector at time $t_{i,j}$, i.e. $\mu(t_{i, j}) = \mathbf{m} \cdot \mathbf{b}(t_{i, j})$ .</p> <p>To model the target distribution $\mathbf{y}_i$ from the observed individual $\mathbf{w}_i$,. we follow the traditional LMM approach via a conditional gaussian:</p> \[\mathbf{y}_i|\mathbf{w}_i \sim \mathcal{N}(\boldsymbol{\mathbf{\mu}}_i + \mathbf{B}_i \cdot \mathbf{w}_i, \sigma^2 \mathbf{I}_{n_i})\] <p>assuming that the target distribution $\mathbf{y}<em>i$ is a sequence of scalar, each scalar value representing the state at $t</em>{i, j}$, $j \in {1, 2, \cdots, n_i}$. Also, $\mathbf{B}<em>i$ is naturally a matrix of dimension $(n_i, K)$ to map the $K$-dim per-sample coefficient $\mathbf{w}_i$ to the same time sequence space. Intuitively, $\mathbf{B}_i = [\mathbf{b}(t</em>{i, 1}), \mathbf{b}(t_{i, 2}), \cdots, \mathbf{b}(t_{i, n_i})]$</p> <h1 id="low-rank-sequential-lmm">Low Rank Sequential LMM</h1> <p>We restrict the latent space to $q &lt; K$ dimensions and learn from the data the mapping $\mathbf{A} \in \mathbb{R}^{K \times q}$ in this reduced dimension LMM:</p> <p>\(\mathbf{y}_i|\mathbf{w}_i \sim \mathcal{N}(\boldsymbol{\mathbf{\mu}}_i + \mathbf{B}_i \cdot A \cdot \mathbf{w}_i, \sigma^2 \mathbf{I}_{n_i})\) where now the individual sample $\mathbf{w}_i$ also reduces its dim to $q$ . Because the optimizations are on $\mathbf{w}_i$ and $\mathbf{A}$ , an EM algorithm can be applied.</p> <h1 id="this-paper">This paper</h1> <p><a href="https://www.researchgate.net/publication/327858691_Longitudinal_data_analysis_using_matrix_completion" rel="external nofollow noopener" target="_blank">original paper</a></p> <p>Instead of using existing ${t_{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ for every sample $i$, the paper defined a universal grid agnostic of actual time series per sample:</p> <p>\(G = [\tau_1, \tau_2, \cdots, \tau_T]\) where $\tau_1$ is the global minimal time across all sample $i$, $\tau_T$ is the global maximal time across all sample $i$, and $T$ is a hyperparameter defining how accurate the grid is. Ideally if $T$ is infinity the grid becomes a continuous space that can capture every $t_{i, j}$.</p> <p>For every $t_{i, j}$ , we locate its corresponding</p> \[g_i(j) = \text{argmin}_{1 \le k \le T}(|\tau_k - t_{i, j}|)\] <p>i.e. the index of $\tau$ that is most close to $t_{i, j}$. We also define $\tilde{\mathbf{y}}<em>{i, j} = \mathbf{y}</em>{i, g_i(j)}$.</p> <p>For sample $i$, now we can map the original time sequence ${t_{i, 1}, t_{i. 2}, \cdots, t_{i, n_i}}$ to ${g_i(1), g_i(2), \cdots, g_i(n_i)}$, and the paper called this new sequence of $g_i$ as $O_i$. For sample $i$, any index in $[1, T]$ that not in $O_i$ is marked as a missing target sample, which is quite natural because $G$ is designed to fit all samples so will always greater than or equal to the time sequence of individual sample. The paper introduces an additional notation $\Omega$ to represent $O_i$ over all samples, i.e. $\Omega = {(i, j): 1 \le i \le N, j \in O_i }$. Finally, the paper reformulated the label space to a $N \times T$ matrix $P_\Omega(\mathbf{Y})$ where:</p> \[P_\Omega(\mathbf{Y})[i, j] = \mathbf{y}_i[O_{i}[j]]\] <p>if target is not missing, 0 otherwise. The operator $P_\Omega(\cdot)$ is a general projection that projects input from space $\Omega$ to the full grid space, by keeping the original value if the input has a corresponding value at that grid position, otherwise fill the value at that grid position with 0.</p> <p>The paper still used the same $\mathbf{b}(t)$ of dim $K$, but $\mathbf{B}_i$ is now replaced by a sample-agnostic $\mathbf{B} = [\mathbf{b}(\tau_1), \mathbf{b}(\tau_2), \cdots, \mathbf{b}(\tau_T)]$ .</p> <p>Again following the same approach we want to minimize the difference between every $\tilde{\mathbf{y}}<em>{i, j}$ and every predicted $\mathbf{w}_i \cdot \mathbf{b}(\tau</em>{g_i(j)})$, i.e.</p> <p>\(\text{argmin}_{\mathbf{w}_i} |\tilde{\mathbf{y}}_{i, j} - \mathbf{w}_i \cdot \mathbf{b}(\tau_{g_i(j)})|\) for all $i$, which is equivalent to the matrix form of:</p> \[\text{argmin}_{\mathbf{W}}||P_\Omega(\mathbf{Y} - \mathbf{W}\cdot \mathbf{B}^T)||_F\] <table> <tbody> <tr> <td>where $\mathbf{W} = [\mathbf{w}_1, \mathbf{w}_2, \cdots, \mathbf{w}_N]$ of dim $(N \times K)$, $\mathbf{B} = [\mathbf{b}(\tau_1), \mathbf{b}(\tau_2), \cdots, \mathbf{b}(\tau_T)]$ of dim $(T, K)$ , and $</td> <td> </td> <td>\cdot</td> <td> </td> <td> <em>F$ being Frobenius norm, i.e. the square root of the sum of matrix elements. Again projection operator $P</em>\Omega$ guarantees that we only keep all available $\mathbf{y}_i$ and skip all missing y in the full grid.</td> </tr> </tbody> </table> <h2 id="soft-longitudinal-impute">Soft-Longitudinal-Impute</h2> <p>To optimize the above loss with missing ys, the paper utilized the fact that optimization of</p> \[\text{argmin}_\mathbf{W} (\frac{1}{2} ||\mathbf{Y} - \mathbf{W} \cdot \mathbf{B}^T||_F^2 + ||\mathbf{W}||_*)\] <table> <tbody> <tr> <td>has a unique solution $\mathbf{W} = S_\lambda(\mathbf{Y}\mathbf{B})$, where $S_\lambda(X) = UD_\lambda V$ and $X = UDV^T$ is the SVD of $X$. Notation: $</td> <td> </td> <td>\cdot</td> <td> </td> <td> <em>*$ is the nuclear norm, i.e. the sum of singular values. $D</em>\lambda$ is a diagonal matrix called soft-thresholding, which does a threshold on the original diagonal matrix $D$ by $D_\lambda[i, i] = \text{max}(D[i, i]-\lambda, 0)$, i.e. subtract $\lambda$ from every diagonal value, keep the subtraction value if it is greater than 0 otherwise set it to 0.</td> </tr> </tbody> </table> <p>Intuitively, this algorithm approaches $\mathbf{W}$ iteratively by a new Y with imputed values to fill in the missing places. For a given approximation of the solution $\mathbf{W}^{\text{old}}$ , we use $\mathbf{W}^{\text{old}}$ to impute unknown elements of $\mathbf{Y}$ obtaining $\tilde{\mathbf{Y}}$ . Then, we construct the next approximation $\mathbf{W}^{\text{new}}$ by running the above SVD based solution on</p> \[\mathbf{W}^{\text{new}} = \text{argmin}_\mathbf{W} (\frac{1}{2} ||\tilde{\mathbf{Y}} - \mathbf{W} \cdot \mathbf{B}^T||_F^2 + ||\mathbf{W}||_*)\] <p>So there wouldn’t be any projection operator.</p> <p>Also because the optimization is sensitive to $\lambda$, the paper proposed that run the optimization with decreasing $\lambda$ to refine the optimization results.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/flows/">Introduction to Normalizing Flows</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/PaperReviewingSelf-SupervisedLearningwithKernelDependenceMaximization/">Paper Reviewing - Self-Supervised Learning with Kernel Dependence Maximization</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/generalized-function/">Generalized funtions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/adam-may-update-frozen-param/">Pytorch Adam may update frozen parameters</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/jekyll-installation-on-macos/">Jekyll Installation on macOS Catalina</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Sidong Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>